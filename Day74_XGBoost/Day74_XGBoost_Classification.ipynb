{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9gQ8FzF7YX2hsLV1pqaDx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# üìò XGBoost Classification\n","---\n","\n","# 1Ô∏è‚É£ Goal\n","Understand how **XGBoost solves classification problems step-by-step**.\n","\n","Toy Problem:\n","Predict **placement (0/1)** using **CGPA**.\n","\n","---\n","\n","# 2Ô∏è‚É£ Prerequisites\n","You should know:\n","- Gradient Boosting\n","- Logistic Regression basics\n","- Log-Odds concept\n","\n","---\n","\n","# 3Ô∏è‚É£ Big Idea\n","\n","XGBoost classification workflow = Gradient Boosting workflow\n","\n","### Core Loop\n","1. Start with base prediction (log-odds)\n","2. Compute residuals\n","3. Train tree on residuals\n","4. Add scaled tree output\n","5. Repeat\n","\n","Key difference:\n","> Trees are built using **similarity score**, not entropy/Gini\n","\n","---\n","\n","# 4Ô∏è‚É£ Stage 1 ‚Äî Base Model\n","\n","In regression ‚Üí mean  \n","In classification ‚Üí **log-odds**\n","\n","### Log-Odds Formula\n","```\n","log(p / (1 - p))\n","```\n","\n","Where:\n","p = probability of positive class\n","\n","---\n","\n","# 5Ô∏è‚É£ Compute Base Log-Odds\n","\n","Assume:\n","- 3 positives\n","- 2 negatives\n","\n","```\n","p = 3/5\n","log-odds = log(3/2) ‚âà 0.405\n","```\n","\n","So base prediction for all samples:\n","```\n","F‚ÇÄ(x) = 0.405\n","```\n","\n","---\n","\n","# 6Ô∏è‚É£ Convert Log-Odds ‚Üí Probability\n","\n","Using sigmoid:\n","\n","```\n","p = e^F / (1 + e^F)\n","```\n","\n","For 0.405:\n","```\n","p ‚âà 0.60\n","```\n","\n","So model predicts:\n","> 60% placement probability for everyone\n","\n","Clearly weak model ‚ùå\n","\n","---\n","\n","# 7Ô∏è‚É£ Residuals (Pseudo-Residuals)\n","\n","Residual formula:\n","```\n","Residual = Actual ‚àí Predicted Probability\n","```\n","\n","Example residuals:\n","- 1 ‚àí 0.6 = +0.4\n","- 0 ‚àí 0.6 = ‚àí0.6\n","\n","Residuals capture:\n","> Model error direction\n","\n","---\n","\n","# 8Ô∏è‚É£ Build First XGBoost Tree\n","\n","Train tree on:\n","```\n","Input: CGPA\n","Target: Residuals\n","```\n","\n","But tree construction is DIFFERENT.\n","\n","---\n","\n","# 9Ô∏è‚É£ Similarity Score (Classification Version)\n","\n","### Formula\n","```\n","Similarity = Œ£(residual¬≤) / Œ£(p * (1 - p)) + Œª\n","```\n","\n","Where:\n","- p = previous probability\n","- Œª = regularization (assume 0)\n","\n","This replaces:\n","- Gini\n","- Entropy\n","\n","---\n","\n","# üîü Root Node Similarity\n","\n","If residuals cancel out ‚Üí score ‚âà 0\n","\n","This becomes baseline.\n","\n","Goal now:\n","> Split data to increase similarity score\n","\n","---\n","\n","# 1Ô∏è‚É£1Ô∏è‚É£ Find Split Points\n","\n","Sort feature (CGPA).\n","\n","Candidate splits = averages between adjacent values.\n","\n","Example:\n","```\n","5.97, 6.67, 7.62, 8.87\n","```\n","\n","We test ALL splits.\n","\n","---\n","\n","# 1Ô∏è‚É£2Ô∏è‚É£ Evaluate Each Split\n","\n","For each split:\n","\n","1. Divide residuals\n","2. Compute similarity for left and right\n","3. Compute gain\n","\n","---\n","\n","# Gain Formula\n","```\n","Gain = LeftScore + RightScore ‚àí ParentScore\n","```\n","\n","Choose split with **max gain**.\n","\n","---\n","\n","# 1Ô∏è‚É£3Ô∏è‚É£ Best Split Found\n","\n","Example best split:\n","```\n","CGPA < 7.62\n","```\n","\n","Tree structure:\n","```\n","        CGPA < 7.62\n","        /            Residual A   Residual B\n","```\n","\n","---\n","\n","# 1Ô∏è‚É£4Ô∏è‚É£ Leaf Output Formula\n","\n","Leaf output determines tree contribution.\n","\n","### Formula\n","```\n","Leaf Output = Œ£(residuals) / Œ£(p * (1 - p)) + Œª\n","```\n","\n","Difference from similarity score:\n","- No square in numerator\n","\n","---\n","\n","# 1Ô∏è‚É£5Ô∏è‚É£ Example Leaf Outputs\n","\n","Left leaf:\n","```\n","‚âà -1.11\n","```\n","\n","Right leaf:\n","```\n","‚âà +1.66\n","```\n","\n","These are in **log-odds space**.\n","\n","---\n","\n","# 1Ô∏è‚É£6Ô∏è‚É£ Stage 2 Model\n","\n","Combined model:\n","```\n","F(x) = Base Log-Odds + Œ∑ √ó Tree Output\n","```\n","\n","Where:\n","Œ∑ = learning rate (e.g., 0.3)\n","\n","---\n","\n","# 1Ô∏è‚É£7Ô∏è‚É£ Example Prediction\n","\n","If CGPA = 6:\n","- Goes left leaf\n","- Output = -1.11\n","\n","New log-odds:\n","```\n","0.405 + 0.3 √ó (-1.11) = 0.072\n","```\n","\n","---\n","\n","# 1Ô∏è‚É£8Ô∏è‚É£ Convert Back to Probability\n","\n","Apply sigmoid again:\n","\n","```\n","p = e^0.072 / (1 + e^0.072) ‚âà 0.518\n","```\n","\n","So probability dropped from:\n","```\n","0.60 ‚Üí 0.52\n","```\n","\n","Model improved ‚úÖ\n","\n","---\n","\n","# 1Ô∏è‚É£9Ô∏è‚É£ Compute New Residuals\n","\n","```\n","New Residual = Actual ‚àí New Probability\n","```\n","\n","Residuals shrink toward 0.\n","\n","This indicates:\n","> Model learning correctly\n","\n","---\n","\n","# 2Ô∏è‚É£0Ô∏è‚É£ Stage 3+\n","\n","Repeat process:\n","\n","1. Train new tree on new residuals\n","2. Add to model\n","3. Convert to probability\n","4. Compute new residuals\n","\n","---\n","\n","# Final Model Form\n","\n","```\n","F(x) = Base + Œ∑T‚ÇÅ + Œ∑T‚ÇÇ + Œ∑T‚ÇÉ ...\n","```\n","\n","Prediction:\n","```\n","Probability = sigmoid(F(x))\n","```\n","\n","---\n","\n","# 2Ô∏è‚É£1Ô∏è‚É£ Key Differences vs Gradient Boosting\n","\n","| Aspect | Gradient Boosting | XGBoost |\n","|-------|------------------|---------|\n","| Tree split metric | Entropy/Gini | Similarity score |\n","| Regularization | Weak | Strong |\n","| Speed | Moderate | Optimized |\n","| Math | Simpler | Second-order |\n","\n","---\n","\n","# 2Ô∏è‚É£2Ô∏è‚É£ Important Formulas\n","\n","### Log-Odds\n","```\n","log(p / (1 - p))\n","```\n","\n","### Sigmoid\n","```\n","e^F / (1 + e^F)\n","```\n","\n","### Similarity Score\n","```\n","Œ£(residual¬≤) / Œ£(p(1 - p))\n","```\n","\n","### Gain\n","```\n","Left + Right ‚àí Parent\n","```\n","\n","### Leaf Output\n","```\n","Œ£(residual) / Œ£(p(1 - p))\n","```\n","\n","---\n","\n","# 2Ô∏è‚É£3Ô∏è‚É£ Intuition\n","\n","- Base model predicts global probability\n","- Trees fix mistakes\n","- Each tree nudges predictions\n","- Residuals shrink toward 0\n","\n","When residual ‚âà 0:\n","> Model is optimal\n","\n","---\n","\n","# 2Ô∏è‚É£4Ô∏è‚É£ Why Log-Odds?\n","\n","Because:\n","- Works well with sigmoid\n","- Stable gradients\n","- Smooth optimization\n","\n","This is same idea used in:\n","- Logistic regression\n","- Gradient boosting classifier\n","\n","---\n","\n","# 2Ô∏è‚É£5Ô∏è‚É£ What Makes XGBoost Powerful\n","\n","- Log-odds optimization\n","- Regularized tree building\n","- Similarity-based splits\n","- Learning rate shrinkage\n","- Additive modeling\n","\n","---\n"],"metadata":{"id":"7WHseulAvSaq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z27jhgsdvJx9"},"outputs":[],"source":[]}]}