
# ğŸ“˜ Linear Regression Assumptions

---

# ğŸ§  Why Assumptions Matter
Linear Regression is not just a formula.
---

# â­ The 5 CORE Assumptions

---

# 1ï¸âƒ£ LINEARITY

## ğŸ“Œ Definition
There must be a **linear relationship** between:
- Independent variables (X)
- Dependent variable (y)

Mathematically:
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + Îµ

---

## ğŸ‘€ Visual Intuition

### âœ… Valid
- Straight upward trend
- Straight downward trend
- Slight noise allowed

### âŒ Invalid
- Curves
- Exponential growth
- Saturation patterns

---

## ğŸ§ª How to Check

### Scatter Plots (Most Important)
```python
import seaborn as sns
import matplotlib.pyplot as plt

for col in X.columns:
    sns.scatterplot(x=X[col], y=y)
    plt.title(f"{col} vs Target")
    plt.show()
```

---

## ğŸš¨ If Violated
Fix using:
- Polynomial regression
- Log transformation
- Non-linear models (Tree, NN)

---

# 2ï¸âƒ£ NO MULTICOLLINEARITY

## ğŸ“Œ Definition
Independent variables must be:
â¡ **NOT strongly correlated with each other**

Bad example:
- Height & Weight (often correlated)
- Experience & Age

---

## ğŸ’£ Why It's Dangerous
Multicollinearity causes:
- Unstable Î² coefficients
- Inflated variance
- Hard interpretation
- Feature importance confusion

---

## ğŸ§  Intuition
If two features carry same info â†’ model gets confused:
â€œWho actually caused the change?â€

---

## ğŸ” Detection Methods

### 1ï¸âƒ£ Correlation Heatmap
```python
sns.heatmap(X.corr(), annot=True, cmap="coolwarm")
```

### Rule:
| Correlation | Meaning |
|------------|--------|
| < 0.5 | Safe |
| 0.5â€“0.8 | Moderate |
| > 0.8 | Risky |

---

### 2ï¸âƒ£ VIF (Most Important)

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

vif = pd.DataFrame()
vif["Feature"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif)
```

### ğŸ¯ VIF Interpretation
| VIF | Meaning |
|-----|--------|
| 1 | Perfect |
| <5 | OK |
| 5â€“10 | Warning |
| >10 | Remove feature |

---

## ğŸ›  Fix Multicollinearity
- Drop features
- PCA
- Ridge regression

---

# 3ï¸âƒ£ NORMALITY OF RESIDUALS

## ğŸ“Œ Residual = Error
Residual = Actual âˆ’ Predicted

Îµ = y âˆ’ Å·

---

## ğŸ“Œ Assumption
Residuals should follow:
â¡ **Normal Distribution (Gaussian)**

---

## ğŸ¤” Why Needed?
Important for:
- Confidence intervals
- p-values
- Hypothesis testing

NOTE:
Not very important for pure prediction.

---

## ğŸ” Visual Checks

### Histogram
```python
sns.histplot(residuals, kde=True)
```

### Q-Q Plot (Most Reliable)
```python
import scipy.stats as stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.show()
```

### âœ… Ideal Q-Q Plot
Points lie on diagonal line.

---

## ğŸ›  Fix If Violated
- Log transform target
- Remove outliers
- Robust regression

---

# 4ï¸âƒ£ HOMOSCEDASTICITY

## ğŸ“Œ Definition
Residual variance should be:
â¡ **Constant across predictions**

Fancy word:
Homoscedasticity = Equal spread

Opposite:
Heteroscedasticity = Unequal spread

---

## ğŸ‘€ Visual Intuition

### âœ… Good
Random cloud

### âŒ Bad
- Funnel shape
- Cone pattern
- Increasing variance

---

## ğŸ” Test Method

### Residual vs Prediction Plot
```python
plt.scatter(y_pred, residuals)
plt.axhline(0, color='red')
plt.xlabel("Predicted")
plt.ylabel("Residuals")
plt.show()
```

---

## ğŸš¨ Why It Matters
Violations cause:
- Wrong confidence intervals
- Biased standard errors
- Invalid inference

---

## ğŸ›  Fix Methods
- Log transformation
- Weighted regression
- Robust regression

---

# 5ï¸âƒ£ NO AUTOCORRELATION

## ğŸ“Œ Definition
Residuals should be:
â¡ Independent of each other

Important for:
- Time series
- Sequential data

---

## âŒ Bad Example
Error today depends on error yesterday.

This breaks independence.

---

## ğŸ” Detection Methods

### Visual
```python
plt.plot(residuals)
plt.title("Residual Sequence")
```

Random pattern = Good

---

### Statistical Test â€” Durbin Watson
```python
from statsmodels.stats.stattools import durbin_watson
durbin_watson(residuals)
```

### ğŸ¯ Interpretation
| Value | Meaning |
|------|--------|
| ~2 | No autocorrelation |
| <1.5 | Positive autocorr |
| >2.5 | Negative autocorr |

---

## ğŸ›  Fix Methods
- Add lag features
- Time series models (ARIMA)
- GLS regression

---

# ğŸ§ª Residual Workflow (COLAB READY)

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
residuals = y_test - y_pred
```

Now run all diagnostics.

---

# ğŸ¯ INTERVIEW CHEAT SHEET

## ğŸ”¥ MUST REMEMBER ORDER
1ï¸âƒ£ Linearity  
2ï¸âƒ£ No multicollinearity  
3ï¸âƒ£ Normal residuals  
4ï¸âƒ£ Homoscedasticity  
5ï¸âƒ£ No autocorrelation  

---

# ğŸ§  Smart Interview Tips

### Q: Most important assumption?
Depends:
- Inference â†’ Normality
- Prediction â†’ Linearity

---

### Q: Most commonly violated?
- Multicollinearity
- Heteroscedasticity

---

### Q: If assumptions fail?
Say:
â€œUse transformation or switch model.â€

Instant + points in interview.

---

# ğŸ§­ Real World Perspective

## In Statistics
Assumptions = CRITICAL

## In Machine Learning
Assumptions = Less strict
But still useful for:
- Explainability
- Feature importance
- Business insights

---
---
