{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5jNn3fvQpKaHNiWXO+Z9w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mathematical Formulation & Implementation of Simple Linear Regression\n","\n","**Based on the CampusX Linear Regression Transcript**\n","\n","---\n","\n","## 1. Introduction and Objective\n","\n","The primary goal of this session is to understand the mathematics behind **Simple Linear Regression** and to build a custom Linear Regression class from scratch in Python.\n","\n","### Core concepts\n","\n","* **Problem:** dataset with one input and one output\n","\n","  * Input variable (X): independent feature (e.g., CGPA)\n","  * Output variable (y): dependent target (e.g., salary package)\n","* **Goal:** find the *best-fit line* that minimizes the distance between the data points and the line.\n","\n","### Line equation\n","\n","[ y = m x + b ]\n","\n","* (y): predicted output (Package)\n","* (x): input feature (CGPA)\n","* (m): slope (weight / coefficient)\n","* (b): intercept (bias)\n","\n","### High-level workflow\n","\n","```mermaid\n","graph LR\n","    A[Input data X, y] --> B[Define line eqn y = m x + b]\n","    B --> C[Define error function E]\n","    C --> D{Minimize error}\n","    D --> E[Find optimal m and b]\n","    E --> F[Best fit line]\n","```\n","\n","---\n","\n","## 2. Approaches to solving Linear Regression\n","\n","There are two primary algorithms to determine optimal (m) and (b).\n","\n","```mermaid\n","graph TD\n","    A[Start: Find m and b] --> B{Dataset size?}\n","    B -- Small / Low Dimensions --> C[Closed form solution]\n","    B -- Large / High Dimensions --> D[Non-closed form solution]\n","    C --> E[Ordinary Least Squares (OLS)]\n","    E --> G[Direct formula calculation]\n","    D --> F[Gradient Descent]\n","    F --> H[Iterative optimization]\n","```\n","\n","### A. Closed-form solution (direct formula)\n","\n","* **Method:** Ordinary Least Squares (OLS) — compute exact solution using derived formulas.\n","* **Library:** `sklearn.linear_model.LinearRegression` implements this.\n","* **Pros:** exact solution, fast for low-dimensional problems.\n","* **Cons:** matrix inverses / solve can be expensive for very high-dimensional data; numerical stability issues may appear for ill-conditioned matrices.\n","\n","### B. Non-closed form solution (approximation)\n","\n","* **Method:** Gradient descent (batch / stochastic / mini-batch).\n","* **Library:** `sklearn.linear_model.SGDRegressor` can be used.\n","* **Pros:** scales to massive datasets and high-dimensional problems; supports online updates.\n","* **Cons:** iterative and approximate; needs tuning (learning rate, iterations, batch size).\n","\n","> **Note:** This document focuses on the **closed-form OLS** method and a from-scratch implementation.\n","\n","---\n","\n","## 3. Mathematical derivation (Ordinary Least Squares)\n","\n","To determine the best line, minimize total squared error between true (y_i) and predictions (\\hat y_i).\n","\n","### Step 1 — Loss function (sum of squared errors)\n","\n","[ E = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2. ]\n","\n","**Why square?**\n","\n","* prevents cancellation of positive/negative errors\n","* differentiable everywhere (smooth), useful for calculus\n","\n","### Step 2 — Substitute the line equation\n","\n","[ \\hat{y}*i = m x_i + b ]\n","[ E(m,b) = \\sum*{i=1}^n (y_i - (m x_i + b))^2. ]\n","\n","This is a function of two variables: (m) and (b).\n","\n","### Step 3 — Partial derivatives and set to zero (first order conditions)\n","\n","**Derivative w.r.t.** (b):\n","\n","[ \\frac{\\partial E}{\\partial b} = \\sum_{i=1}^n 2(y_i - m x_i - b)(-1) = 0 ]\n","\n","Simplify:\n","\n","[ \\sum_{i=1}^n (y_i - m x_i - b) = 0 ]\n","[ \\sum y_i - m \\sum x_i - n b = 0 ]\n","[ \\frac{1}{n} \\sum y_i - m \\frac{1}{n} \\sum x_i - b = 0 ]\n","[ b = \\bar{y} - m \\bar{x} ]\n","\n","**Derivative w.r.t.** (m):\n","\n","[ \\frac{\\partial E}{\\partial m} = \\sum_{i=1}^n 2(y_i - m x_i - b)(-x_i) = 0 ]\n","\n","Substitute (b = \\bar{y} - m\\bar{x}) and rearrange; after algebraic simplification we get:\n","\n","[ m = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}. ]\n","\n","So final closed-form formulas are:\n","\n","* **Slope:** ( m = \\dfrac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} )\n","* **Intercept:** ( b = \\bar{y} - m \\bar{x} )\n","\n","---\n","\n","## 4. Final formulas (summary)\n","\n","Use these to implement linear regression from scratch:\n","\n","* ( m = \\dfrac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} )\n","* ( b = \\bar{y} - m\\bar{x} )\n","\n","---\n","\n","## 5. Python implementation — building `MeraLR`\n","\n","### Algorithm flow (fit method)\n","\n","```mermaid\n","graph TD\n","    A[Start fit] --> B[Calc mean of X and Y]\n","    B --> C[Initialize numerator=0, denominator=0]\n","    C --> D[Loop over data points]\n","    D --> E{any more points?}\n","    E -- Yes --> F[add (xi-x_mean)*(yi-y_mean) to numerator]\n","    F --> G[add (xi-x_mean)^2 to denominator]\n","    G --> D\n","    E -- No --> H[compute m = numerator/denominator]\n","    H --> I[compute b = y_mean - m * x_mean]\n","    I --> J[End fit]\n","```\n","\n","### Implementation (vectorized + simple loop version)\n","\n","```python\n","import numpy as np\n","\n","class MeraLR:\n","    def __init__(self):\n","        # parameters\n","        self.m = None\n","        self.b = None\n","\n","    def fit(self, X_train, y_train):\n","        \"\"\"\n","        Trains the model by calculating m and b.\n","        X_train: 1-D numpy array or shape (n_samples,) or (n_samples,1)\n","        y_train: 1-D numpy array (n_samples,)\n","        \"\"\"\n","        # ensure 1-D arrays\n","        X = np.ravel(X_train)\n","        y = np.ravel(y_train)\n","\n","        # 1. Means\n","        x_bar = X.mean()\n","        y_bar = y.mean()\n","\n","        # 2. Numerator and denominator\n","        numerator = 0.0\n","        denominator = 0.0\n","        for i in range(X.shape[0]):\n","            numerator += (X[i] - x_bar) * (y[i] - y_bar)\n","            denominator += (X[i] - x_bar) ** 2\n","\n","        # 3. slope and intercept\n","        self.m = numerator / denominator\n","        self.b = y_bar - (self.m * x_bar)\n","\n","        print(f\"Training complete. m={self.m}, b={self.b}\")\n","\n","    def predict(self, X_test):\n","        \"\"\"Predict using y = m*x + b. Accepts scalars or arrays.\"\"\"\n","        X = np.ravel(X_test)\n","        return self.m * X + self.b\n","```\n","\n","> **Notes:**\n",">\n","> * The code above assumes `denominator != 0` (i.e., not all `x` identical). Handle edge-cases in production.\n","> * Vectorized NumPy implementations are faster: compute numerator as `((X-x_bar) * (y-y_bar)).sum()` and denominator with `((X-x_bar)**2).sum()`.\n","\n","---\n","\n","## 6. Line-by-line explanation (fit method)\n","\n","* `numerator = 0`, `denominator = 0`: running totals for the slope formula.\n","* `x_bar = X_train.mean()`: sample mean (\\bar{x}).\n","* `y_bar = y_train.mean()`: sample mean (\\bar{y}).\n","* Loop: build sums\n","\n","  * `numerator += (X[i] - x_bar) * (y[i] - y_bar)` builds (\\sum (x-\\bar{x})(y-\\bar{y}))\n","  * `denominator += (X[i] - x_bar) ** 2` builds (\\sum (x-\\bar{x})^2)\n","* `self.m = numerator / denominator`: slope.\n","* `self.b = y_bar - (self.m * x_bar)`: intercept.\n","\n","---\n","\n","## 7. Comparison & conclusion\n","\n","* **Verification:** On a typical placement dataset the custom `MeraLR` produces the same `m` and `b` as `sklearn.linear_model.LinearRegression` (within numerical precision).\n","* **Limitation:** This implementation handles **simple** linear regression (single feature). For multiple features the closed-form generalizes to matrix form (Normal Equation):\n","\n","[ \\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y} ]\n","\n","* **Scalability & numerical concerns:** For high-dimensional data use stable solvers (`np.linalg.lstsq`, SVD) or iterative solvers; add regularization (Ridge) to avoid singular `X^T X`.\n","\n","### Key takeaway\n","\n","Simple Linear Regression is basic calculus + statistics: compute slope and intercept that minimize the sum of squared errors. It’s interpretable, fast for low dimensional problems, but limited in expressivity — use multiple regression, polynomial features, or non-linear models if needed.\n","\n","---\n","\n","## 8. Quick runnable example (verify with sklearn)\n","\n","```python\n","# toy verify\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","\n","# synthetic data\n","X = np.array([6.6,7.5,8.1,5.9,7.0,8.4,6.8,9.0,5.5,7.8]).reshape(-1,1)\n","y = np.array([3.0,4.2,4.5,2.5,3.7,4.6,3.2,5.1,2.2,4.0])\n","\n","# our model\n","mylr = MeraLR()\n","mylr.fit(X, y)\n","print('Predictions (ours):', mylr.predict(X[:3]))\n","\n","# sklearn\n","lr = LinearRegression().fit(X, y)\n","print('Sklearn m, b:', lr.coef_[0], lr.intercept_)\n","```\n","\n","---\n","\n","## 9. Practical tips\n","\n","* **Edge cases:** if all `x` equal → denominator = 0 → no unique slope. Handle explicitly.\n","* **Numerical stability:** use `np.linalg.lstsq` or SVD for matrix solutions.\n","* **Multiple features:** implement matrix normal equation or use `sklearn`.\n","* **Regularization:** add Ridge/Lasso for better generalization when features are many or correlated.\n","\n","---\n","\n","## 10. References & next steps\n","\n","* Scikit-learn docs: `LinearRegression`, `SGDRegressor`, `PolynomialFeatures`.\n","* Implement gradient descent for learning: compare training\n"],"metadata":{"id":"gxJJE7e_3UR-"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"A4ySfPnU66Oa","executionInfo":{"status":"ok","timestamp":1765347956784,"user_tz":-330,"elapsed":16,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}}},"outputs":[],"source":["class MeraLR:\n","\n","    def __init__(self):\n","        self.m = None\n","        self.b = None\n","\n","    def fit(self,X_train,y_train):\n","\n","        num = 0\n","        den = 0\n","\n","        for i in range(X_train.shape[0]):\n","\n","            num = num + ((X_train[i] - X_train.mean())*(y_train[i] - y_train.mean()))\n","            den = den + ((X_train[i] - X_train.mean())*(X_train[i] - X_train.mean()))\n","\n","        self.m = num/den\n","        self.b = y_train.mean() - (self.m * X_train.mean())\n","        print(self.m)\n","        print(self.b)\n","\n","    def predict(self,X_test):\n","\n","        print(X_test)\n","\n","        return self.m * X_test + self.b\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","df = pd.read_csv('placement.csv')\n","df.head()\n","\n","X = df.iloc[:,0].values\n","y = df.iloc[:,1].values\n"],"metadata":{"id":"NuySBNU37itz","executionInfo":{"status":"ok","timestamp":1765347956814,"user_tz":-330,"elapsed":15,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n","X_train.shape\n","\n","lr = MeraLR()\n","lr.fit(X_train,y_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcZZlnfo7ord","executionInfo":{"status":"ok","timestamp":1765347956879,"user_tz":-330,"elapsed":17,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"7623959e-7f5f-4301-e862-af6a50c00191"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5579519734250721\n","-0.8961119222429152\n"]}]},{"cell_type":"code","source":["X_train.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7v1zO7MN7wWM","executionInfo":{"status":"ok","timestamp":1765347956888,"user_tz":-330,"elapsed":6,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"10f11cf2-4c7a-4a43-cc52-178d1a8778d5"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["160"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["X_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wu04zHg70gz","executionInfo":{"status":"ok","timestamp":1765347956905,"user_tz":-330,"elapsed":14,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"0da06f0e-f8c3-4184-912d-192855768f9a"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(7.14)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["X_train.mean()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pqSRQdSn72V4","executionInfo":{"status":"ok","timestamp":1765347956912,"user_tz":-330,"elapsed":5,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"f39ad5b7-4368-4bef-fb17-1b5b0a6d81bc"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(6.989937500000001)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["X_test[0]\n","\n","print(lr.predict(X_test[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3LvBcSt74Yc","executionInfo":{"status":"ok","timestamp":1765347956924,"user_tz":-330,"elapsed":11,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"95977fd3-0ef2-4512-d0ab-bf131a14a80a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["8.58\n","3.891116009744203\n"]}]}]}