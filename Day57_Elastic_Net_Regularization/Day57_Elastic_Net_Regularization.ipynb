{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5w5ndpqpWow+v4IZGhV1o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Elastic Net Regularization — Complete Mathematical & Conceptual Guide (Arnav Tomar copyright)\n","\n","---\n","\n","## 1. Motivation: Why Elastic Net Exists\n","\n","Regularization is used to control model complexity and reduce overfitting.\n","\n","Two classical approaches:\n","\n","- Ridge Regression (L2): handles multicollinearity but keeps all features\n","- Lasso Regression (L1): performs feature selection but is unstable with correlated features\n","\n","Elastic Net is introduced to **combine the strengths of both**.\n","\n","---\n","\n","## 2. Linear Regression (Baseline)\n","\n","Model:\n","\n","$$\n","\\hat{y} = Xw + b\n","$$\n","\n","Mean Squared Error loss:\n","\n","$$\n","J(w) = \\frac{1}{n}\\|y - Xw\\|_2^2\n","$$\n","\n","Problem: overfitting when features are many or correlated.\n","\n","---\n","\n","## 3. Ridge Regression (L2)\n","\n","Objective function:\n","\n","$$\n","J_{\\text{ridge}}(w) = \\frac{1}{n}\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_2^2\n","$$\n","\n","Key properties:\n","\n","- Shrinks coefficients\n","- Never sets coefficients exactly to zero\n","- Stable under multicollinearity\n","\n","Closed-form solution:\n","\n","$$\n","w = (X^TX + \\lambda I)^{-1}X^Ty\n","$$\n","\n","---\n","\n","## 4. Lasso Regression (L1)\n","\n","Objective function:\n","\n","$$\n","J_{\\text{lasso}}(w) = \\frac{1}{n}\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1\n","$$\n","\n","Key properties:\n","\n","- Produces sparse solutions\n","- Performs feature selection\n","- Unstable when features are highly correlated\n","\n","No closed-form solution due to non-differentiability at zero.\n","\n","---\n","\n","## 5. Limitation of Ridge and Lasso\n","\n","### Ridge limitation:\n","- Cannot remove irrelevant features\n","\n","### Lasso limitation:\n","- Selects only one feature from a correlated group\n","- Selection becomes unstable\n","\n","Elastic Net solves both.\n","\n","---\n","\n","## 6. Elastic Net Objective Function\n","\n","Elastic Net combines L1 and L2 penalties:\n","\n","$$\n","J_{\\text{EN}}(w) =\n","\\frac{1}{n}\\|y - Xw\\|_2^2\n","+ \\lambda \\left(\n","\\alpha \\|w\\|_1\n","+ (1 - \\alpha)\\|w\\|_2^2\n","\\right)\n","$$\n","\n","Where:\n","\n","- $$\\lambda \\ge 0$$ → overall regularization strength\n","- $$0 \\le \\alpha \\le 1$$ → mixing parameter\n","\n","---\n","\n","## 7. Special Cases\n","\n","### When $$\\alpha = 0$$\n","\n","$$\n","J_{\\text{EN}} = \\text{Ridge Regression}\n","$$\n","\n","### When $$\\alpha = 1$$\n","\n","$$\n","J_{\\text{EN}} = \\text{Lasso Regression}\n","$$\n","\n","### When $$0 < \\alpha < 1$$\n","\n","$$\n","\\text{Elastic Net}\n","$$\n","\n","---\n","\n","## 8. Alternative Parameterization (Library Form)\n","\n","Many libraries use:\n","\n","$$\n","\\lambda = \\lambda_1 + \\lambda_2\n","$$\n","\n","$$\n","\\alpha = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\n","$$\n","\n","Penalty becomes:\n","\n","$$\n","\\lambda_1 \\|w\\|_1 + \\lambda_2 \\|w\\|_2^2\n","$$\n","\n","---\n","\n","## 9. Why Elastic Net Works for Multicollinearity\n","\n","Given two highly correlated features:\n","\n","$$\n","x_1 \\approx x_2\n","$$\n","\n","- Lasso tends to select **one** and discard the other\n","- Ridge shrinks both but keeps both\n","- Elastic Net **selects both together and shrinks them**\n","\n","This is called the **grouping effect**.\n","\n","---\n","\n","## 10. Geometry Intuition\n","\n","### Lasso constraint:\n","\n","$$\n","\\|w\\|_1 \\le c\n","$$\n","\n","→ diamond-shaped  \n","→ sharp corners → sparsity\n","\n","### Ridge constraint:\n","\n","$$\n","\\|w\\|_2^2 \\le c\n","$$\n","\n","→ circular  \n","→ smooth → no sparsity\n","\n","### Elastic Net constraint:\n","\n","$$\n","\\alpha \\|w\\|_1 + (1-\\alpha)\\|w\\|_2^2 \\le c\n","$$\n","\n","→ rounded diamond  \n","→ sparsity + stability\n","\n","---\n","\n","## 11. Optimization Properties\n","\n","- Convex objective\n","- Non-differentiable at zero due to L1 term\n","- Solved using:\n","  - Coordinate Descent\n","  - Proximal Gradient Descent\n","  - SGD with elastic penalty\n","\n","---\n","\n","## 12. Bias–Variance Tradeoff\n","\n","As $\\lambda$ increases:\n","\n","$$\n","\\text{Bias} \\uparrow\n","$$\n","\n","$$\n","\\text{Variance} \\downarrow\n","$$\n","\n","Elastic Net allows smoother control compared to pure Lasso.\n","\n","---\n","\n","## 13. Feature Selection Behavior\n","\n","Elastic Net:\n","\n","- Can set coefficients exactly to zero\n","- Keeps correlated important features together\n","- Reduces dimensionality safely\n","\n","---\n","\n","## 14. When to Use Elastic Net\n","\n","Use Elastic Net when:\n","\n","- Number of features $$p \\gg n$$\n","- Strong multicollinearity exists\n","- Feature importance is unknown\n","- Need both stability and sparsity\n","\n","---\n","\n","## 15. Interview-Ready Comparison\n","\n","| Method | Shrinkage | Feature Selection | Multicollinearity |\n","|------|----------|------------------|------------------|\n","| Ridge | Yes | No | Excellent |\n","| Lasso | Yes | Yes | Poor |\n","| Elastic Net | Yes | Yes | Excellent |\n","\n","---\n","\n","## 16. One-Line Interview Answer\n","\n","> Elastic Net combines L1 and L2 penalties to achieve both feature selection and coefficient stability, especially effective when features are highly correlated.\n","\n","---\n","\n","## 17. Key Takeaways\n","\n","- Elastic Net generalizes Ridge and Lasso\n","- Controlled by $\\lambda$ and $\\alpha$\n","- Produces sparse yet stable models\n","- Preferred for high-dimensional correlated data\n","\n","---\n","\n","## 18. Memory Formula\n","\n","$$\n","\\text{Elastic Net} = \\text{Lasso} + \\text{Ridge}\n","$$\n","\n","$$\n","\\alpha \\rightarrow \\text{L1 vs L2 balance}\n","$$\n","\n","$$\n","\\lambda \\rightarrow \\text{penalty strength}\n","$$\n","---"],"metadata":{"id":"2gcr3oL_zINL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uy4KuUFnzGaU","executionInfo":{"status":"ok","timestamp":1766218162409,"user_tz":-330,"elapsed":2589,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"bc929f0e-b713-4d61-d533-048b7612f80a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4399338661568968"]},"metadata":{},"execution_count":1}],"source":["from sklearn.datasets import load_diabetes\n","from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","X,y = load_diabetes(return_X_y=True)\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"]},{"cell_type":"code","source":["# Linear Regression\n","reg = LinearRegression()\n","reg.fit(X_train,y_train)\n","y_pred = reg.predict(X_test)\n","r2_score(y_test,y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzezYuwAzjIp","executionInfo":{"status":"ok","timestamp":1766218162438,"user_tz":-330,"elapsed":17,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"a9ca48b1-4de6-48fa-b1fd-135bdff23d0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4399338661568968"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Ridge\n","reg = Ridge(alpha=0.1)\n","reg.fit(X_train,y_train)\n","y_pred = reg.predict(X_test)\n","r2_score(y_test,y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSmNhgnuzlTR","executionInfo":{"status":"ok","timestamp":1766218187145,"user_tz":-330,"elapsed":42,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"da97abfa-df49-43e1-ea88-3d7453f6afce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.45199494197195456"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Lasso\n","reg = Lasso(alpha=0.01)\n","reg.fit(X_train,y_train)\n","y_pred = reg.predict(X_test)\n","r2_score(y_test,y_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsDsbYh9zp7Z","executionInfo":{"status":"ok","timestamp":1766218203685,"user_tz":-330,"elapsed":10,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"044884da-2658-417e-b17d-cc3877572640"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.44111855963110613"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# ElasticNet\n","# arnav\n","reg = ElasticNet(alpha=0.005,l1_ratio=0.9)\n","reg.fit(X_train,y_train)\n","y_pred = reg.predict(X_test)\n","r2_score(y_test,y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3WZIy74zwCk","executionInfo":{"status":"ok","timestamp":1766218206167,"user_tz":-330,"elapsed":16,"user":{"displayName":"Arnav Tomar","userId":"05362157737552244045"}},"outputId":"912839fc-6bdf-4ace-a158-c58f0b58cb3a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4531474541554823"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"-GpgqEWHzwpA"},"execution_count":null,"outputs":[]}]}