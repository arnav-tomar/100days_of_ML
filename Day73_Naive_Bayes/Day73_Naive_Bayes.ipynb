{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORePf9iQJAulT8EJYMRmAN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# üìò Naive Bayes\n","---\n","\n","# üß† 1. What is Naive Bayes?\n","\n","Naive Bayes is a **probabilistic classification algorithm** based on Bayes Theorem.\n","\n","It predicts class using:\n","\n","$$\n","P(C \\mid X)\n","$$\n","\n","Where:\n","- $C$ = class  \n","- $X = (x_1, x_2, ..., x_n)$ = features  \n","\n","Assumption:\n","Features are conditionally independent given the class.\n","\n","---\n","\n","# üßÆ 2. Bayes Theorem\n","\n","$$\n","P(C \\mid X) = \\frac{P(C)\\,P(X \\mid C)}{P(X)}\n","$$\n","\n","In practice:\n","\n","$$\n","P(C \\mid X) \\propto P(C)\\,P(X \\mid C)\n","$$\n","\n","---\n","\n","# üß† 3. Naive Independence Assumption\n","\n","$$\n","P(x_1, x_2, ..., x_n \\mid C)\n","=\n","\\prod_{i=1}^{n} P(x_i \\mid C)\n","$$\n","\n","Final working formula:\n","\n","$$\n","P(C \\mid X) \\propto P(C)\\prod P(x_i \\mid C)\n","$$\n","\n","---\n","\n","# ‚öôÔ∏è 4. Working Steps\n","\n","## Step 1 ‚Äî Prior\n","\n","$$\n","P(C) = \\frac{\\text{class count}}{\\text{total samples}}\n","$$\n","\n","## Step 2 ‚Äî Likelihood\n","\n","$$\n","P(x_i \\mid C)\n","$$\n","\n","## Step 3 ‚Äî Multiply\n","\n","$$\n","Score(C) = P(C)\\prod P(x_i \\mid C)\n","$$\n","\n","## Step 4 ‚Äî Predict\n","\n","$$\n","\\hat{y} = \\arg\\max_C Score(C)\n","$$\n","\n","---\n","\n","# üìä 5. Types of Naive Bayes\n","\n","## Gaussian NB (Continuous Data)\n","\n","$$\n","P(x \\mid C) =\n","\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n","\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n","$$\n","\n","---\n","\n","## Multinomial NB (Text / Counts)\n","\n","Used for:\n","- NLP\n","- Spam detection\n","- Document classification\n","\n","---\n","\n","## Bernoulli NB (Binary Features)\n","\n","$$\n","x_i \\in \\{0,1\\}\n","$$\n","\n","---\n","\n","# üß† 6. Log Trick (Very Important)\n","\n","To avoid numerical underflow:\n","\n","$$\n","\\log P(C \\mid X) =\n","\\log P(C) + \\sum \\log P(x_i \\mid C)\n","$$\n","\n","This is used in real implementations.\n","\n","---\n","\n","# üß™ 7. Example Logic\n","\n","Given class priors:\n","\n","$$\n","P(Yes)=0.5,\\quad P(No)=0.5\n","$$\n","\n","Compute likelihoods and compare scores:\n","\n","$$\n","Score(Yes) \\quad vs \\quad Score(No)\n","$$\n","\n","Choose maximum.\n","\n","---\n"],"metadata":{"id":"14YtBNKwhdJU"}},{"cell_type":"markdown","source":["Covers:\n","- Problem solving using Bayes\n","- Intuition\n","- Mathematics behind Naive Bayes\n","- Code implementation\n","- Handling numerical data\n","\n","---\n","\n","# üß† 1. Bayes Theorem Refresher\n","\n","$$\n","P(C \\mid X) = \\frac{P(C) P(X \\mid C)}{P(X)}\n","$$\n","\n","Used for classification:\n","\n","$$\n","P(C \\mid X) \\propto P(C) P(X \\mid C)\n","$$\n","\n","---\n","\n","# üßÆ 2. Problem Solving Using Bayes\n","\n","Typical format:\n","\n","Given:\n","- Priors\n","- Conditional probabilities\n","\n","Find:\n","\n","$$\n","P(C_i \\mid X)\n","$$\n","\n","Steps:\n","1. Compute prior\n","2. Compute likelihood\n","3. Multiply\n","4. Compare scores\n","\n","---\n","\n","# üß† 3. Intuition of Naive Bayes\n","\n","Model asks:\n","\n","> How likely is this class?  \n","> How likely are these features in that class?\n","\n","Then combines them.\n","\n","Decision rule:\n","\n","$$\n","\\hat{y} = \\arg\\max_C P(C) \\prod P(x_i \\mid C)\n","$$\n","\n","---\n","\n","# ‚ö° 4. Why It Works Surprisingly Well\n","\n","Even with wrong independence assumption:\n","- Probabilities still rank classes correctly\n","- Works great in high dimensions\n","- Strong baseline model\n","\n","---\n","\n","# üßÆ 5. Mathematical Derivation\n","\n","Start from:\n","\n","$$\n","P(C \\mid X) = \\frac{P(C, X)}{P(X)}\n","$$\n","\n","Using conditional probability:\n","\n","$$\n","P(C, X) = P(C)P(X \\mid C)\n","$$\n","\n","Apply independence assumption:\n","\n","$$\n","P(X \\mid C) = \\prod_{i=1}^n P(x_i \\mid C)\n","$$\n","\n","Final:\n","\n","$$\n","P(C \\mid X) \\propto P(C) \\prod P(x_i \\mid C)\n","$$\n","\n","---\n","\n","# üß† 6. Log Likelihood Form\n","\n","To avoid underflow:\n","\n","$$\n","\\log P(C \\mid X) =\n","\\log P(C) + \\sum \\log P(x_i \\mid C)\n","$$\n","\n","Used in real implementations.\n","\n","---\n","\n","# üß™ 7. Simple Example\n","\n","Given two classes:\n","Spam vs Not Spam\n","\n","Sentence:\n","\"Win money now\"\n","\n","Compute:\n","\n","$$\n","Score(Spam) = P(Spam) \\prod P(word \\mid Spam)\n","$$\n","\n","Compare with:\n","\n","$$\n","Score(NotSpam)\n","$$\n","\n","Choose higher.\n","\n","---\n","\n","# üõ†Ô∏è 8. Handling Numerical Data\n","\n","For continuous features ‚Üí Gaussian NB\n","\n","Assume normal distribution:\n","\n","$$\n","x \\sim \\mathcal{N}(\\mu, \\sigma^2)\n","$$\n","\n","Likelihood:\n","\n","$$\n","P(x \\mid C) =\n","\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n","\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n","$$\n","\n","---\n","\n","# üß† 9. Parameter Estimation\n","\n","For each class:\n","\n","$$\n","\\mu = \\text{mean of feature}\n","$$\n","\n","$$\n","\\sigma^2 = \\text{variance}\n","$$\n","\n","Computed from training data.\n","\n","---\n","\n","# üíª 10. Code Implementation (Scikit-learn)\n","\n","```python\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\n","\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","pred = model.predict(X_test)\n","print(accuracy_score(y_test, pred))\n","```\n","\n","---\n","\n","# üß† 11. When to Use Which Variant\n","\n","| Data Type | NB Variant |\n","|----------|------------|\n","| Continuous | Gaussian |\n","| Word counts | Multinomial |\n","| Binary features | Bernoulli |\n","\n","---\n","\n","# ‚ö° 12. Strengths\n","\n","- Extremely fast\n","- Works well with small data\n","- Great for text classification\n","- Easy baseline model\n","\n","---\n","\n","# ‚ùå 13. Limitations\n","\n","- Independence assumption unrealistic\n","- Correlated features hurt performance\n","- Probability calibration poor\n","\n","---\n","\n","# üéØ 14. Interview Questions\n","\n","**Why naive?**  \n","Assumes feature independence.\n","\n","**Why log probabilities?**  \n","Avoid numerical underflow.\n","\n","**Why good for NLP?**  \n","High dimensional sparse data.\n","\n","**Gaussian NB assumption?**  \n","Features normally distributed.\n","\n","---\n","\n","# üßæ Final Formula\n","\n","$$\n","\\hat{y} =\n","\\arg\\max_C\n","\\left[\n","\\log P(C) + \\sum \\log P(x_i \\mid C)\n","\\right]\n","$$\n","\n","---"],"metadata":{"id":"OjBZooREj6Sy"}},{"cell_type":"markdown","source":["# üß† 15. Text Classification Example\n","\n","Sentence: ‚Äúfree money now‚Äù\n","\n","$$\n","P(Spam \\mid words) \\propto\n","P(Spam)\\prod P(word_i \\mid Spam)\n","$$\n","\n","---\n","\n","# üõ†Ô∏è 16. Zero Probability Problem\n","\n","If any likelihood = 0 ‚Üí product becomes 0.\n","\n","Solution: **Laplace Smoothing**\n","\n","$$\n","P(x \\mid C) =\n","\\frac{\\text{count}+1}{\\text{total}+V}\n","$$\n","\n","Where $V$ = vocabulary size.\n","\n","---\n","\n","# ‚ö° 17. Advantages\n","\n","- Very fast  \n","- Works with small data  \n","- Great for NLP  \n","- Easy to implement  \n","\n","---\n","\n","# ‚ùå 18. Disadvantages\n","\n","- Unrealistic independence assumption  \n","- Sensitive to correlated features  \n","- Poor probability calibration  \n","\n","---\n","\n","# üìä 19. Applications\n","\n","- Spam filtering  \n","- Sentiment analysis  \n","- Fake news detection  \n","- Medical diagnosis  \n","- Document classification  \n","\n","---\n","\n","# üß† 20. Naive Bayes vs Logistic Regression\n","\n","| Feature | Naive Bayes | Logistic Regression |\n","|--------|-------------|--------------------|\n","| Speed | Very fast | Medium |\n","| Data needed | Small | Large |\n","| Handles correlation | Poor | Better |\n","\n","---\n","\n","# üß™ 21. Scikit-learn Implementation\n","\n","```python\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\n","\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","pred = model.predict(X_test)\n","```\n","\n","---\n","\n","# üß† 22. Interview Questions\n","\n","**Why naive?**  \n","Because of independence assumption.\n","\n","**Why logs?**  \n","Avoid numerical underflow.\n","\n","**Best use case?**  \n","Text classification.\n","\n","**When not to use?**  \n","Highly correlated features.\n","\n","---\n","\n","# üßæ Final Formula to Remember\n","\n","$$\n","\\hat{y} =\n","\\arg\\max_C\n","\\left[\n","\\log P(C) + \\sum \\log P(x_i \\mid C)\n","\\right]\n","$$\n","\n","---\n","\n","# ‚úÖ Final Summary\n","\n","Naive Bayes is:\n","- A simple probabilistic classifier\n","- Based on Bayes theorem\n","- Uses independence assumption\n","- Extremely powerful for NLP tasks\n","\n","Despite simplicity, it performs surprisingly well in real-world scenarios."],"metadata":{"id":"dJosCKnkkN4J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRAVqRZ-hTIY"},"outputs":[],"source":[]}]}