{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEAhNctd68vcEWnVYsL96y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Logistic Regression ‚Äî Improving Perceptron Using Probabilistic Updates\n","\n","---\n","\n","## 00:00‚Äì01:36 ‚Äî Recap: What Was the Problem with Perceptron?\n","\n","We are studying **Logistic Regression**.\n","\n","In the previous videos, we studied the **Perceptron Trick** and identified a **fundamental flaw** in its algorithm.\n","\n","### Perceptron Logic (Recap)\n","\n","- Run a loop for many iterations\n","- Randomly pick a data point\n","- Ask:\n","  - Is this point **misclassified**?\n","    - Yes ‚Üí move the line toward the point\n","    - No ‚Üí do nothing\n","\n","Eventually, we get a line that separates both classes.\n","\n","### Problem\n","\n","Once **all points are correctly classified**, the algorithm **stops learning**.\n","\n","- Even if the margin is poor\n","- Even if the decision boundary can be improved\n","\n","But **Logistic Regression** continues to improve and finds a **better-balanced boundary**.\n","\n","üëâ The flaw is **not in data**, it is **in the algorithm itself**.\n","\n","---\n","\n","## 01:36‚Äì02:43 ‚Äî Core Idea: Change the Algorithm\n","\n","Earlier:\n","- Only **misclassified points** affected the line\n","\n","Now:\n","- **Every point** will affect the line\n","\n","### New Strategy\n","\n","- Misclassified points ‚Üí **pull** the line toward themselves\n","- Correctly classified points ‚Üí **push** the line away from themselves\n","\n","So learning **never stops**, even after perfect classification.\n","\n","---\n","\n","## 02:43‚Äì05:00 ‚Äî Distance-Based Influence (Key Insight)\n","\n","The **magnitude of push or pull** depends on **distance from the line**.\n","\n","### Misclassified Points\n","\n","- Close to line ‚Üí small pull\n","- Far from line ‚Üí strong pull\n","\n","### Correctly Classified Points\n","\n","- Close to line ‚Üí strong push\n","- Far from line ‚Üí weak push\n","\n","This ensures:\n","- Boundary moves until it reaches a **stable central position**\n","- Similar behavior to Logistic Regression\n","\n","---\n","\n","## 05:00‚Äì07:21 ‚Äî Summary of the New Learning Rule\n","\n","For **every point**:\n","\n","- Check if point is:\n","  - Correctly classified ‚Üí push\n","  - Incorrectly classified ‚Üí pull\n","- Strength of update depends on:\n","  - Distance from decision boundary\n","\n","This creates a **continuous learning dynamic**.\n","\n","---\n","\n","## 07:21‚Äì11:33 ‚Äî Revisiting the Perceptron Update Equation\n","\n","Original Perceptron update:\n","\n","$$\n","\\mathbf{w}_{new} = \\mathbf{w}_{old} + \\eta (y - \\hat{y}) \\mathbf{x}\n","$$\n","\n","### Four Cases\n","\n","| True $y$ | Predicted $\\hat{y}$ | $(y - \\hat{y})$ | Effect |\n","|--------|------------------|---------------|--------|\n","| 1 | 1 | 0 | No update |\n","| 0 | 0 | 0 | No update |\n","| 1 | 0 | 1 | Pull |\n","| 0 | 1 | -1 | Push |\n","\n","‚ùå **Problem**  \n","When $(y - \\hat{y}) = 0$, the update becomes **zero** ‚Üí learning stops.\n","\n","---\n","\n","## 11:33‚Äì14:51 ‚Äî Why Step Function Breaks Learning\n","\n","Prediction was made using a **step function**:\n","\n","$$\n","\\hat{y} =\n","\\begin{cases}\n","1 & z \\ge 0 \\\\\n","0 & z < 0\n","\\end{cases}\n","$$\n","\n","Where:\n","\n","$$\n","z = \\mathbf{w} \\cdot \\mathbf{x}\n","$$\n","\n","### Issue\n","\n","- Output is **discrete**: $0$ or $1$\n","- Causes $(y - \\hat{y}) = 0$\n","- Update becomes zero\n","- Learning halts\n","\n","üëâ We must **replace the step function**\n","\n","---\n","\n","## 14:51‚Äì17:42 ‚Äî Introducing the Sigmoid Function\n","\n","We replace the step function with the **sigmoid function**:\n","\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","### Properties\n","\n","- Output range: $(0, 1)$\n","- Smooth and continuous\n","- Never exactly $0$ or $1$\n","\n","Key values:\n","\n","- $z \\to +\\infty \\Rightarrow \\sigma(z) \\to 1$\n","- $z \\to -\\infty \\Rightarrow \\sigma(z) \\to 0$\n","- $z = 0 \\Rightarrow \\sigma(z) = 0.5$\n","\n","---\n","\n","## 17:42‚Äì20:44 ‚Äî New Prediction Mechanism\n","\n","Now prediction is:\n","\n","$$\n","\\hat{y} = \\sigma(\\mathbf{w} \\cdot \\mathbf{x})\n","$$\n","\n","Instead of discrete labels, we get:\n","\n","$$\n","\\hat{y} \\in (0,1)\n","$$\n","\n","Interpretation:\n","\n","- $\\hat{y}$ = **probability of positive class**\n","\n","---\n","\n","## 20:44‚Äì23:34 ‚Äî Probabilistic Interpretation\n","\n","Let:\n","\n","- Event $A$: Placement happens\n","\n","Then:\n","\n","$$\n","P(A) = \\hat{y}\n","$$\n","\n","And:\n","\n","$$\n","P(\\text{No Placement}) = 1 - \\hat{y}\n","$$\n","\n","### Decision Boundary Meaning\n","\n","- On the line: $z = 0 \\Rightarrow \\hat{y} = 0.5$\n","- Above the line ‚Üí higher probability of placement\n","- Below the line ‚Üí lower probability\n","\n","The entire space becomes a **probability gradient**.\n","\n","---\n","\n","## 23:34‚Äì26:21 ‚Äî Why Sigmoid Fixes the Perceptron Problem\n","\n","Now:\n","\n","$$\n","y - \\hat{y} \\neq 0\n","$$\n","\n","So update never vanishes.\n","\n","### New Update Rule\n","\n","$$\n","\\mathbf{w}_{new} = \\mathbf{w}_{old} + \\eta (y - \\sigma(z)) \\mathbf{x}\n","$$\n","\n","- Correctly classified points still update\n","- Misclassified points update more strongly\n","- Distance controls magnitude automatically\n","\n","---\n","\n","## 26:21‚Äì30:28 ‚Äî Example with Four Points\n","\n","Assume four points with sigmoid outputs:\n","\n","| Point | $\\hat{y}$ |\n","|-----|----------|\n","| P1 | 0.80 |\n","| P2 | 0.65 |\n","| P3 | 0.30 |\n","| P4 | 0.15 |\n","\n","Then:\n","\n","$$\n","y - \\hat{y} \\neq 0 \\quad \\forall \\text{ points}\n","$$\n","\n","So **every point contributes** to learning.\n","\n","---\n","\n","## 30:28‚Äì34:17 ‚Äî Distance-Based Magnitude (Mathematical Proof)\n","\n","- Larger $|z|$ ‚Üí sigmoid closer to $0$ or $1$\n","- Smaller $|z|$ ‚Üí sigmoid closer to $0.5$\n","\n","Hence:\n","\n","- Near boundary ‚Üí strong updates\n","- Far from boundary ‚Üí weak updates\n","\n","Exactly what we wanted.\n","\n","---\n","\n","## 34:17‚Äì39:40 ‚Äî Experimental Result\n","\n","Three models compared:\n","\n","- Red line ‚Üí Pure Perceptron (step function)\n","- Brown line ‚Üí Modified Perceptron + Sigmoid\n","- Black line ‚Üí True Logistic Regression\n","\n","Result:\n","\n","- Brown > Red (improvement achieved)\n","- Brown < Black (still not perfect)\n","\n","Meaning:\n","- Direction is correct\n","- Solution is not complete yet\n","\n","---\n","\n","## 39:40‚Äì40:32 ‚Äî Conclusion\n","\n","- Replacing step function with sigmoid:\n","  - Prevents zero updates\n","  - Enables continuous learning\n","  - Introduces probability interpretation\n","- Algorithm improves significantly\n","- Still not fully Logistic Regression\n","\n","üëâ Remaining gap will be fixed using:\n","- Proper loss function\n","- Gradient Descent\n","\n","**Next video completes the model.**\n","\n","---\n"],"metadata":{"id":"v3WT2EnOTzBw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCSFlsDBQaGN"},"outputs":[],"source":[]}]}