<div align="center">

# ğŸ¤– 100 Days of Machine Learning

### *From Zero to Real AI Systems â€” A Builder's Journey*

![ML Banner](https://img.shields.io/badge/Challenge-100%20Days%20of%20ML-blueviolet?style=for-the-badge&logo=python&logoColor=white)
![Status](https://img.shields.io/badge/Status-Completed%20âœ…-success?style=for-the-badge)
![Focus](https://img.shields.io/badge/Focus-Build%20%7C%20Ship%20%7C%20Learn-orange?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-ML-orange?style=for-the-badge&logo=scikit-learn&logoColor=white)

<br/>

> *"Consistency beats motivation. Simpler models often win. Data quality > model complexity."*

<br/>

[![GitHub stars](https://img.shields.io/github/stars/ArnavTomar18/100days_of_ML?style=social)](https://github.com/ArnavTomar18/100days_of_ML)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?logo=linkedin)](www.linkedin.com/in/arnavtomar18)

</div>

---

## ğŸ§­ Table of Contents

| Section | Description |
|--------|-------------|
| [ğŸ¯ Why This Exists](#-why-this-challenge-exists) | The philosophy behind the challenge |
| [ğŸ§  What I Learned](#-what-i-covered) | Full knowledge breakdown |
| [ğŸ“ Math Foundations](#-math-for-intuition) | Applied math for ML |
| [âš™ï¸ Algorithms Built](#ï¸-algorithms-implemented) | Every algorithm, hands-on |
| [ğŸ” ML Workflow](#-model-building-workflow) | Repeatable system I developed |
| [ğŸ› ï¸ Projects](#ï¸-real-projects-built) | OTT Recommender + more |
| [ğŸ§ª Stack](#-tools--stack) | Full tech stack |
| [ğŸ“ˆ Outcomes](#-key-outcomes) | What I actually gained |
| [ğŸ’¡ Real Lessons](#-lessons-that-actually-matter) | Honest takeaways |
| [ğŸš€ What's Next](#-whats-next) | Future roadmap |

---

## ğŸ¯ Why This Challenge Exists

> Most people consume ML content passively. This challenge was built to do the **exact opposite.**

```
âŒ Watch tutorials      â†’    âœ… Build real systems
âŒ Passive learning     â†’    âœ… Implementation-first
âŒ Theory only          â†’    âœ… Real dataset intuition
âŒ Student mindset      â†’    âœ… Builder mindset
âŒ Certificates         â†’    âœ… Shipped projects
```

The mission was simple: **100 days. No excuses. Build or fail.**

---

## ğŸ§  What I Covered

<details>
<summary><b>ğŸ“¦ 1. Core Python & Data Foundations</b> â€” click to expand</summary>

<br/>

| Tool | Use Case |
|------|----------|
| ğŸ Python | Core ML scripting & workflows |
| ğŸ”¢ NumPy | Array operations, linear algebra |
| ğŸ¼ Pandas | Data wrangling, manipulation |
| ğŸ“Š Matplotlib | Static visualizations |
| ğŸŒŠ Seaborn | Statistical plotting |
| ğŸ““ Jupyter | Notebook experimentation |

**Key Skills â€” with actual days:**
- Reading data from CSV, JSON, MySQL, APIs â†’ [Day 15](./Day15_pandas_diffway_readcsv/), [Day 16](./Day16_Working%20_with_JSON_MySql/), [Day 17](./Day17_API_to_Dataframe/)
- Web scraping into DataFrames â†’ [Day 18](./Day18_webscraping/)
- Missing value handling (CCA, KNN, MICE) â†’ [Day 35â€“40](./Day35_Handling_Missing_Data_Complete_Case_Analysis/)
- Categorical encoding (Label, One-Hot, Column Transformer) â†’ [Day 26â€“28](./Day26_Categorical/)
- Feature scaling & normalization â†’ [Day 24â€“25](./Day24_scaling_standardization/)
- Outlier removal (Z-Score, IQR, Winsorization) â†’ [Day 41â€“44](./Day41-42_Outliers_Removal_using_Z-scor_Method/)
- Power transforms, Binning, Date/Time features â†’ [Day 31â€“34](./Day31_Power_transformer/)

</details>

<details>
<summary><b>ğŸ” 2. Exploratory Data Analysis (EDA)</b> â€” click to expand</summary>

<br/>

- ğŸ“‰ Univariate distribution analysis â†’ [Day 20](./Day20_Univariate_Analysis/)
- ğŸ”— Full EDA workflow â†’ [Day 21](./Day21_EDA/)
- ğŸ› ï¸ Automated EDA tools â†’ [Day 22](./Day22_EDA_tool/)
- ğŸ¯ Understanding data â€” types, shapes, stats â†’ [Day 19](./Day19_Undaerstanding_Data/)
- ğŸ§¹ Identifying dataset biases and patterns
- ğŸ’¡ Visualization-driven storytelling

</details>

---

## ğŸ“ Math for Intuition

> Not textbook theory â€” applied understanding that actually matters in code.

```
ğŸ”· Linear Algebra       â†’  Matrix ops, dot products, vector spaces
ğŸ”· Probability          â†’  Distributions, Bayes theorem intuition  
ğŸ”· Statistics           â†’  Descriptive stats, correlation, variance
ğŸ”· Gradient Descent     â†’  How models actually learn  [Day 51â€“52]
ğŸ”· Loss Functions       â†’  MSE, Cross-entropy, and when to use what [Day 49, 59]
ğŸ”· Regularization       â†’  Ridge, Lasso, Elastic Net  [Day 55â€“57]
ğŸ”· Bias-Variance        â†’  The core ML tension         [Day 54]
ğŸ”· PCA / Eigenvectors   â†’  Dimensionality intuition    [Day 47â€“48]
```

---

## âš™ï¸ Algorithms Implemented

> Every algorithm was built with **real datasets + evaluation + failure analysis**

### ğŸ“ˆ Regression

| Algorithm | Day | Key Learnings |
|-----------|-----|---------------|
| Linear Regression (Multiple) | [Day 50](./Day50_Multiple_Linear_Regression/) | Cost function, normal equation, from scratch |
| Polynomial Regression | [Day 53](./Day53_Polynomial_Regression/) | Non-linearity, degree tuning, overfitting risk |
| Ridge Regression (L2) | [Day 55](./Day55_Ridge_Regression/) | Regularization, shrinking coefficients |
| Lasso Regression (L1) | [Day 56](./Day56_%20Lasso_Regression/) | Feature selection via sparsity |
| Elastic Net | [Day 57](./Day57_Elastic_Net_Regularization/) | Combining L1 + L2 for balance |

### ğŸ·ï¸ Classification

| Algorithm | Day | Key Learnings |
|-----------|-----|---------------|
| Logistic Regression | [Day 58](./Day58_Logistic_Regression/) | Sigmoid, log loss, decision boundary |
| Logistic Reg (Multi-class) | [Day 60](./Day60_Logistic_Regression_Non-Linear_Data_and_Multi-class/) | OvR strategy, non-linear data |
| K-Nearest Neighbors | [Day 70](./Day70_k-Nearest_Neighbors/) | Distance metrics, K tuning |
| Decision Tree | [Day 61](./Day61_Decision_Tree/) | Gini impurity, tree depth, pruning |
| Random Forest | [Day 64](./Day64_Random_Forest/) | Bagging, feature importance |
| AdaBoost | [Day 65](./Day65_Adaboost_Classifier/) | Weak learners â†’ strong classifier |
| Gradient Boosting | [Day 67](./Day67_Gradient_Boosting_Classification/) | Sequential error correction |
| XGBoost | [Day 74](./Day74_XGBoost/) | Regularized boosting, speed |
| Stacking & Blending | [Day 68](./Day68_Stacking_Blending/) | Meta-learning, advanced ensembles |
| Support Vector Machine | [Day 72](./Day72_Support_Vector_Machine/) | Margin maximization, kernel trick |
| Naive Bayes | [Day 73](./Day73_Naive_Bayes/) | Conditional independence, probabilities |

### ğŸ”µ Unsupervised / Clustering

| Algorithm | Day | Key Learnings |
|-----------|-----|---------------|
| K-Means Clustering | [Day 66](./Day66_K-Means_Clustering/) | Centroids, inertia, elbow method |
| Agglomerative Clustering | [Day 69](./Day69_Agglomerative_Clustering/) | Hierarchical merging, dendrograms |
| DBSCAN | [Day 75](./Day75_DBSCAN_Clustering/) | Density-based, noise detection |

### ğŸ“ Dimensionality Reduction

| Technique | Day | Key Learnings |
|-----------|-----|---------------|
| PCA (Intuition) | [Day 47](./Day47_PCA_Geometric_Intuition/) | Eigenvectors, variance explained |
| PCA (Applied) | [Day 48](./Day48_PCA_Example/) | Real dataset dimensionality reduction |
| Curse of Dimensionality | [Day 46](./Day46_Curse_of_Dimensionality/) | Why high dimensions hurt performance |

### ğŸ¯ Evaluation Metrics Used

```python
âœ… Accuracy          âœ… Precision & Recall       âœ… F1 Score
âœ… ROC-AUC Curve     âœ… Confusion Matrix          âœ… Mean Squared Error
âœ… RÂ² Score          âœ… Cross-Validation Score    âœ… Bias-Variance Analysis
```
â†’ Deep dive: [Day 49 â€” Regression Metrics](./Day49_Regression_Evaluation_Metrics/) | [Day 59 â€” Classification Metrics](./Day59_Classification_Metrics/)

---

## ğŸ” Model Building Workflow

> A standardized, repeatable pipeline developed through 100 days of iteration.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  1ï¸âƒ£  Data Understanding    â†’  Know your data first  â”‚
â”‚  2ï¸âƒ£  Cleaning & EDA        â†’  Garbage in, garbage outâ”‚
â”‚  3ï¸âƒ£  Feature Engineering   â†’  Where the magic is   â”‚
â”‚  4ï¸âƒ£  Model Selection       â†’  Start simple, always  â”‚
â”‚  5ï¸âƒ£  Train / Test Split    â†’  Never leak the future â”‚
â”‚  6ï¸âƒ£  Evaluation            â†’  More than accuracy    â”‚
â”‚  7ï¸âƒ£  Iteration             â†’  Debug â†’ improve loop  â”‚
â”‚  8ï¸âƒ£  Optimization          â†’  Tune only after eval  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Core Concepts Deeply Understood:**

- âš–ï¸ Overfitting vs Underfitting â€” and how to diagnose both
- ğŸ¯ Biasâ€“Variance Tradeoff â€” the fundamental tension in ML
- ğŸ”„ Cross-Validation â€” K-Fold, Stratified, and when to use each
- ğŸ† Feature Importance â€” what actually drives predictions
- ğŸ”¬ Model Comparison â€” fair evaluation frameworks

---

## ğŸ› ï¸ Real Projects Built

### ğŸ¬ OTT Recommendation System

> A personalized movie & show recommendation engine built from scratch.

```
ğŸ“Œ What it does:
   â€¢ Filters content by genre, platform, rating
   â€¢ Guided selection logic for user preferences
   â€¢ Personalized recommendation output
   â€¢ Platform-aware content filtering

ğŸ”§ Built with:
   â€¢ Pandas (data filtering & manipulation)
   â€¢ Content-based filtering logic
   â€¢ Interactive selection flow
```

---

### ğŸ§ª ML Experiments Repository

> A collection of end-to-end algorithm notebooks across multiple domains.

```
ğŸ“Œ Includes:
   â€¢ Regression experiments with real datasets
   â€¢ Classification benchmarks across algorithms
   â€¢ Feature impact & importance studies
   â€¢ Model comparison notebooks
```

---

### ğŸ“Š Data Analysis Mini-Projects

> EDA-heavy notebooks that go deep on real-world messy datasets.

```
ğŸ“Œ Includes:
   â€¢ Visualization-heavy insight extraction
   â€¢ Correlation and distribution studies
   â€¢ Bias and anomaly detection
   â€¢ Pattern recognition from raw data
```

---

## ğŸ§ª Tools & Stack

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?style=for-the-badge&logo=python&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white)
![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)

</div>

---

## ğŸ“ Repository Structure

> Each day = one focused concept + notebook + mini-experiment

```
ArnavTomar18/100days_of_ML/
â”‚
â”œâ”€â”€ ğŸ“‚ Day13-14/                              # Early data foundations
â”‚
â”œâ”€â”€ ğŸ“‚ Day15_pandas_diffway_readcsv/          # Pandas CSV reading methods
â”œâ”€â”€ ğŸ“‚ Day16_Working_with_JSON_MySql/         # JSON & MySQL data sources
â”œâ”€â”€ ğŸ“‚ Day17_API_to_Dataframe/                # Pulling API data into DataFrames
â”œâ”€â”€ ğŸ“‚ Day18_webscraping/                     # Web scraping pipelines
â”‚
â”œâ”€â”€ ğŸ“‚ Day19_Understanding_Data/              # â”€â”€ EDA PHASE â”€â”€
â”œâ”€â”€ ğŸ“‚ Day20_Univariate_Analysis/             # Single feature distributions
â”œâ”€â”€ ğŸ“‚ Day21_EDA/                             # Full EDA workflow
â”œâ”€â”€ ğŸ“‚ Day22_EDA_tool/                        # Automated EDA tools
â”‚
â”œâ”€â”€ ğŸ“‚ Day23_Feature_Engineering/             # â”€â”€ FEATURE ENGINEERING â”€â”€
â”œâ”€â”€ ğŸ“‚ Day24_scaling_standardization/         # StandardScaler, MinMaxScaler
â”œâ”€â”€ ğŸ“‚ Day25_Normalization/                   # Normalization techniques
â”œâ”€â”€ ğŸ“‚ Day26_Categorical/                     # Categorical encoding intro
â”œâ”€â”€ ğŸ“‚ Day27_OneHotEncoding/                  # One-Hot Encoding deep dive
â”œâ”€â”€ ğŸ“‚ Day28_Column_Transformer/              # sklearn ColumnTransformer
â”œâ”€â”€ ğŸ“‚ Day29_Pickle_pipeline/                 # Saving ML pipelines
â”œâ”€â”€ ğŸ“‚ Day30_Function_Transformer/            # Custom transformers
â”œâ”€â”€ ğŸ“‚ Day31_Power_transformer/               # Box-Cox, Yeo-Johnson
â”œâ”€â”€ ğŸ“‚ Day32_Binning_and_Binarization/        # Discretization techniques
â”œâ”€â”€ ğŸ“‚ Day33_Handling_Mixed_Variables/        # Mixed-type columns
â”œâ”€â”€ ğŸ“‚ Day34_Handling_Date_and_Time/          # Datetime feature extraction
â”‚
â”œâ”€â”€ ğŸ“‚ Day35_Handling_Missing_Data_CCA/       # â”€â”€ MISSING DATA â”€â”€
â”œâ”€â”€ ğŸ“‚ Day36_Numeric_Features/               # Univariate imputation
â”œâ”€â”€ ğŸ“‚ Day37_Handling_Categorical_Missing/    # Categorical imputation
â”œâ”€â”€ ğŸ“‚ Day38_Random_Imputation/              # Random sample imputation
â”œâ”€â”€ ğŸ“‚ Day39_KNN_Imputer_Multivariate/       # KNN-based imputation
â”œâ”€â”€ ğŸ“‚ Day40_Iterative_Imputer_MICE/         # MICE algorithm
â”‚
â”œâ”€â”€ ğŸ“‚ Day41-42_Outliers_Z-Score/            # â”€â”€ OUTLIERS â”€â”€
â”œâ”€â”€ ğŸ“‚ Day43_Removal_IQR_Method/             # IQR-based removal
â”œâ”€â”€ ğŸ“‚ Day44_Percentile_Winsorization/       # Capping / Winsorization
â”‚
â”œâ”€â”€ ğŸ“‚ Day45_Feature_Construction_Splitting/  # â”€â”€ DIMENSIONALITY â”€â”€
â”œâ”€â”€ ğŸ“‚ Day46_Curse_of_Dimensionality/        # Why high-dim data hurts
â”œâ”€â”€ ğŸ“‚ Day47_PCA_Geometric_Intuition/        # PCA â€” the visual way
â”œâ”€â”€ ğŸ“‚ Day48_PCA_Example/                    # PCA applied
â”‚
â”œâ”€â”€ ğŸ“‚ Day49_Regression_Evaluation_Metrics/  # â”€â”€ REGRESSION â”€â”€
â”œâ”€â”€ ğŸ“‚ Day50_Multiple_Linear_Regression/     # MLR from scratch
â”œâ”€â”€ ğŸ“‚ Day51_Gradient_Descent/              # GD mechanics
â”œâ”€â”€ ğŸ“‚ Day52_Types_of_Gradient_Descent/     # Batch, SGD, Mini-batch
â”œâ”€â”€ ğŸ“‚ Day53_Polynomial_Regression/         # Non-linear regression
â”œâ”€â”€ ğŸ“‚ Day54_Bias_Variance_Tradeoff/        # The core ML tension
â”œâ”€â”€ ğŸ“‚ Day55_Ridge_Regression/             # L2 regularization
â”œâ”€â”€ ğŸ“‚ Day56_Lasso_Regression/             # L1 regularization
â”œâ”€â”€ ğŸ“‚ Day57_Elastic_Net_Regularization/   # L1 + L2 combined
â”‚
â”œâ”€â”€ ğŸ“‚ Day58_Logistic_Regression/          # â”€â”€ CLASSIFICATION â”€â”€
â”œâ”€â”€ ğŸ“‚ Day59_Classification_Metrics/       # Precision, Recall, F1, ROC
â”œâ”€â”€ ğŸ“‚ Day60_Logistic_Regression_NonLinear/# Multi-class logistic reg
â”œâ”€â”€ ğŸ“‚ Day61_Decision_Tree/               # Tree-based learning
â”œâ”€â”€ ğŸ“‚ Day62_Intro_Ensemble_Learning/     # Why ensembles work
â”œâ”€â”€ ğŸ“‚ Day63_Bagging_Ensemble/            # Bootstrap aggregating
â”œâ”€â”€ ğŸ“‚ Day64_Random_Forest/              # Random Forest deep dive
â”œâ”€â”€ ğŸ“‚ Day65_Adaboost_Classifier/        # Adaptive boosting
â”œâ”€â”€ ğŸ“‚ Day67_Gradient_Boosting/          # GBM classification
â”œâ”€â”€ ğŸ“‚ Day68_Stacking_Blending/          # Advanced ensemble methods
â”œâ”€â”€ ğŸ“‚ Day70_k-Nearest_Neighbors/        # KNN full implementation
â”œâ”€â”€ ğŸ“‚ Day72_Support_Vector_Machine/     # SVM + kernels
â”œâ”€â”€ ğŸ“‚ Day73_Naive_Bayes/               # Probabilistic classifier
â”œâ”€â”€ ğŸ“‚ Day74_XGBoost/                   # XGBoost implementation
â”‚
â”œâ”€â”€ ğŸ“‚ Day66_K-Means_Clustering/        # â”€â”€ UNSUPERVISED â”€â”€
â”œâ”€â”€ ğŸ“‚ Day69_Agglomerative_Clustering/  # Hierarchical clustering
â”œâ”€â”€ ğŸ“‚ Day75_DBSCAN_Clustering/        # Density-based clustering
â”‚
â”œâ”€â”€ ğŸ“‚ Day71_Linear_Regression_Assumptions/ # Statistical deep-dives
â”‚
â”œâ”€â”€ ğŸ“„ train.csv                         # Shared training dataset
â””â”€â”€ ğŸ“„ README.md                         # You are here â† 
```

---

## ğŸ“ˆ Key Outcomes

```
âœ… Strong hands-on ML foundations            (not just theory)
âœ… Implementation confidence                 (can build from scratch)
âœ… Real messy dataset experience             (not clean Kaggle toys)
âœ… End-to-end ML pipeline development        (idea â†’ trained model)
âœ… Repeatable experimentation workflow       (professional approach)
âœ… Multiple shipped projects                 (proof of work)
âœ… Algorithm-level debugging ability         (real understanding)
```

---

## ğŸ’¡ Lessons That Actually Matter

> Hard-won lessons from 100 days of building, failing, and iterating.

```
ğŸ’  Consistency beats motivation             â€” Show up even on bad days
ğŸ’  Simpler models often win                 â€” Resist the urge to over-engineer
ğŸ’  Data quality > model complexity          â€” Fix your data first, always
ğŸ’  Debugging teaches more than tutorials    â€” Break things intentionally
ğŸ’  Shipping compounds learning              â€” Done > perfect
ğŸ’  EDA is not optional                      â€” You don't know your data yet
ğŸ’  Metrics lie if you choose the wrong one  â€” Accuracy â‰  model quality
ğŸ’  Overfitting is a symptom, not the disease â€” Understand root causes
```

---

## ğŸš§ Limitations (Being Honest)

This challenge focused on **classical machine learning**. The following are not deeply covered here (yet):

```
âš ï¸  Deep learning architectures
âš ï¸  Large-scale distributed training
âš ï¸  Production-grade MLOps
âš ï¸  Real-time inference systems
âš ï¸  Transformer / LLM internals
```

These are intentional gaps â€” and they're all in the next phase.

---

## ğŸš€ What's Next

> The foundation is built. Now it's time to scale.

```
ğŸ”œ  Deep Learning with PyTorch
ğŸ”œ  Transformer-based models & fine-tuning
ğŸ”œ  Recommendation systems at scale
ğŸ”œ  Model deployment pipelines (FastAPI, Docker)
ğŸ”œ  MLOps fundamentals (experiment tracking, CI/CD for ML)
ğŸ”œ  AI-powered full-stack applications
ğŸ”œ  Startup-level AI product development
```

---

## ğŸ¤ Connect With Me

<div align="center">

[![Portfolio](https://img.shields.io/badge/Portfolio-FF5722?style=for-the-badge&logo=google-chrome&logoColor=white)]((https://portfolio-steel-one-88.vercel.app/))
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in/arnavtomar18)
[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ArnavTomar18/100days_of_ML)

</div>

---

<div align="center">

## â­ If This Helped You

**Give it a star** â€” it helps more builders discover this resource.

![Star Banner](https://img.shields.io/badge/â­_Star_this_repo-_it_helps!-yellow?style=for-the-badge)

<br/>

*Built with ğŸ’œ across 100 days of consistent effort*

*"The best time to start was Day 1. The second best time is now."*

</div>
