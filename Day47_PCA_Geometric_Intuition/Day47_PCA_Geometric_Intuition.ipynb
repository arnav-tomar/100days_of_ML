{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) ‚Äì Intuition, Math, and Usage\n",
        "\n",
        "## 0. Big Picture\n",
        "\n",
        "When your dataset has **too many features (columns)**:\n",
        "\n",
        "- Training becomes slow.\n",
        "- Models overfit more easily.\n",
        "- Distances in high dimensions become weird (curse of dimensionality).\n",
        "- Visualizing data is impossible beyond 3D.\n",
        "\n",
        "To fix this, we use **dimensionality reduction**.\n",
        "\n",
        "There are two main strategies:\n",
        "\n",
        "1. **Feature Selection** ‚Äì keep *some* of the original features, drop the rest.  \n",
        "2. **Feature Extraction** ‚Äì create *new* features from the original ones (combinations / transformations).\n",
        "\n",
        "üëâ **PCA (Principal Component Analysis)** is a **feature extraction** technique for **unsupervised** dimensionality reduction.\n",
        "\n",
        "- Input: only **X (features)**, no y (labels).\n",
        "- Output: a new set of features called **principal components**.\n",
        "- Goal: **reduce dimensions while preserving as much information (variance) as possible**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Curse of Dimensionality (Quick Recap)\n",
        "\n",
        "Suppose you keep adding more and more features:\n",
        "\n",
        "- Up to some point, new features help your model.\n",
        "- After that point, adding more noisy / redundant features:\n",
        "  - does **not** improve performance,\n",
        "  - may **reduce** performance,\n",
        "  - increases computational cost,\n",
        "  - increases risk of overfitting.\n",
        "\n",
        "So there is an **optimal number of features**, beyond which you only get **cost but no benefit**.\n",
        "\n",
        "Dimensionality reduction tries to:\n",
        "\n",
        "- Remove useless/redundant features.\n",
        "- Or compress them into fewer, more informative features.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Feature Selection vs Feature Extraction\n",
        "\n",
        "### 2.1 Feature Selection (Keep a Subset of Original Features)\n",
        "\n",
        "You **choose some original columns** and drop others.\n",
        "\n",
        "Example dataset:\n",
        "\n",
        "- `rooms`  ‚Äì number of rooms in a house\n",
        "- `grocery_shops` ‚Äì number of grocery shops near the house\n",
        "- `price`  ‚Äì price of the house (target)\n",
        "\n",
        "Intuitively:\n",
        "\n",
        "- `rooms` clearly affects `price` (more rooms ‚Üí higher price).\n",
        "- `grocery_shops` might matter a bit, but not as strongly.\n",
        "\n",
        "So if you must keep **only one** feature:\n",
        "- You will keep **`rooms`**, and drop **`grocery_shops`**.\n",
        "\n",
        "This is **feature selection**: choose `rooms`, drop `grocery_shops`.\n",
        "\n",
        "#### Geometric intuition for feature selection\n",
        "\n",
        "Plot a scatter:\n",
        "\n",
        "- x-axis: `rooms`\n",
        "- y-axis: `grocery_shops`\n",
        "\n",
        "You project all points onto each axis:\n",
        "\n",
        "- Projection on `rooms` ‚Üí spread is **large**.\n",
        "- Projection on `grocery_shops` ‚Üí spread is **small**.\n",
        "\n",
        "The axis with **larger spread (variance)** carries more information about how data points differ.\n",
        "\n",
        "So you **keep the feature with higher variance** ‚Üí `rooms`.\n",
        "\n",
        "That‚Äôs essentially what many feature selection methods do:\n",
        "- Prefer features with high variance / stronger relationship with the target.\n",
        "\n",
        "### 2.2 Where Feature Selection Fails\n",
        "\n",
        "Change the second feature:\n",
        "\n",
        "- `rooms`\n",
        "- `bathrooms`\n",
        "- `price`\n",
        "\n",
        "Now:\n",
        "\n",
        "- `price` depends on **both** `rooms` and `bathrooms`.\n",
        "- If you drop either `rooms` or `bathrooms`, you lose important information.\n",
        "\n",
        "Geometrically:\n",
        "\n",
        "- If you plot `rooms` vs `bathrooms`, the points lie roughly along a **diagonal line**:\n",
        "  - houses with more rooms usually have more bathrooms.\n",
        "- The spread along `rooms` and along `bathrooms` axes is now **similar**.\n",
        "- So simple variance comparison cannot clearly say ‚Äúkeep only rooms‚Äù or ‚Äúkeep only bathrooms‚Äù.\n",
        "\n",
        "Feature selection here is awkward: both features matter and are correlated.\n",
        "\n",
        "You need something smarter.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Feature Extraction ‚Äì The Core Idea\n",
        "\n",
        "Instead of choosing one of `{rooms, bathrooms}`, you can define a **new feature**:\n",
        "\n",
        "- `size_of_flat = some_function(rooms, bathrooms)`\n",
        "\n",
        "Example conceptual idea:\n",
        "\n",
        "- More rooms + more bathrooms ‚Üí larger flat.\n",
        "- So `size_of_flat` summarizes both.\n",
        "\n",
        "Now your dataset is:\n",
        "\n",
        "- `size_of_flat`\n",
        "- `price`\n",
        "\n",
        "You replaced two correlated features with **one combined feature** that still carries the important information.\n",
        "\n",
        "This is **feature extraction**:  \n",
        "Create new features from old ones.\n",
        "\n",
        "üëâ **PCA does exactly this**, but in a **systematic, mathematical** way for any number of dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. What PCA Actually Does\n",
        "\n",
        "Given a dataset with **d features**:\n",
        "\n",
        "- PCA computes **d new axes** (directions) called **principal components (PCs)**.\n",
        "- These are:\n",
        "  - **Linear combinations** of original features.\n",
        "  - **Orthogonal** to each other (uncorrelated).\n",
        "  - Sorted by how much **variance** they capture.\n",
        "```latex\n",
        "If your original features are \\( x_1, x_2, ..., x_d \\),\n",
        "\n",
        "each principal component \\( \\text{PC}_k \\) is of the form:\n",
        "\n",
        "\\[\n",
        "\\text{PC}_k = a_{k1} x_1 + a_{k2} x_2 + \\dots + a_{kd} x_d\n",
        "\\]\n",
        "\n",
        "Where vector \\( a_k = (a_{k1},...,a_{kd}) \\) is a **direction** in feature space.\n",
        "\n",
        "```\n",
        "- **PC1**: direction with **maximum variance**.\n",
        "- **PC2**: direction with **maximum variance** subject to being **orthogonal to PC1**.\n",
        "- etc.\n",
        "\n",
        "Then you:\n",
        "\n",
        "- **Keep only the first K components** (`K < d`),\n",
        "- Drop the remaining ones,\n",
        "- Work with the transformed data in K dimensions.\n",
        "\n",
        "So PCA:\n",
        "\n",
        "- **Transforms** the coordinate system (rotates it),\n",
        "- **Keeps the high-variance directions**,\n",
        "- **Throws away low-variance directions** (assumed mostly noise/redundancy).\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Geometric Intuition with Rooms/Bathrooms Example\n",
        "\n",
        "Original 2D coordinates:\n",
        "\n",
        "- x-axis: `rooms`\n",
        "- y-axis: `bathrooms`\n",
        "\n",
        "Points lie roughly along a diagonal:\n",
        "\n",
        "```text\n",
        "^ bathrooms\n",
        "|\n",
        "|      *\n",
        "|    *   *\n",
        "|  *       *\n",
        "|*___________> rooms\n",
        "\n",
        "\n",
        "```\n",
        "PCA will:\n",
        "\n",
        "- Rotate the axes so that:\n",
        "  - **PC1** lies along the direction of **maximum spread** (the diagonal).\n",
        "  - **PC2** is **perpendicular** to PC1.\n",
        "\n",
        "So the new axes become:\n",
        "\n",
        "- **PC1** ‚âà ‚Äúoverall size / space‚Äù of the flat  \n",
        "  (a linear combination of `rooms + bathrooms`)\n",
        "- **PC2** ‚âà small leftover variation  \n",
        "  (noise or minor deviations from perfect correlation)\n",
        "\n",
        "Now:\n",
        "\n",
        "- **Variance along PC1** is very large.\n",
        "- **Variance along PC2** is small.\n",
        "\n",
        "So we can:\n",
        "\n",
        "- **Keep only PC1**, **drop PC2**.\n",
        "\n",
        "We reduced **2D ‚Üí 1D**.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- We converted two correlated features (`rooms`, `bathrooms`) into one main feature: a **size-like** direction.\n",
        "- That‚Äôs **feature extraction via PCA** (not just dropping columns, but creating a new, more informative axis).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Why PCA Cares About Variance\n",
        "\n",
        "You repeatedly saw statements like:\n",
        "\n",
        "- ‚ÄúPick axis with maximum spread / variance‚Äù\n",
        "- ‚ÄúMaximize variance along principal components‚Äù\n",
        "\n",
        "**Question:** Why is variance so important?\n",
        "\n",
        "### 6.1 Mean vs Variance ‚Äì Quick Recap\n",
        "\n",
        "Given data points \\( x_1, x_2, \\dots, x_n \\):\n",
        "\n",
        "- **Mean:**\n",
        "\n",
        "  \\[\n",
        "  \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
        "  \\]\n",
        "\n",
        "- **Variance:**\n",
        "\n",
        "  \\[\n",
        "  \\operatorname{Var}(X) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
        "  \\]\n",
        "\n",
        "- **Standard deviation:**\n",
        "\n",
        "  \\[\n",
        "  \\sigma = \\sqrt{\\operatorname{Var}(X)}\n",
        "  \\]\n",
        "\n",
        "Two different datasets can have the **same mean** but very different **variance** (spread).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Dataset A: \\(-5,\\; 0,\\; +5\\)\n",
        "- Dataset B: \\(-10,\\; 0,\\; +10\\)\n",
        "\n",
        "Both have mean \\(= 0\\), but:\n",
        "\n",
        "- Dataset B is **more spread out** ‚Üí **larger variance**.\n",
        "\n",
        "**Takeaway:**\n",
        "\n",
        "- Mean ‚Üí tells you the **center**.\n",
        "- Variance ‚Üí tells you **how spread out** values are around the center.\n",
        "\n",
        "\n",
        "### 6.2 Variance as ‚ÄúInformation‚Äù\n",
        "\n",
        "In PCA, we want to keep directions where:\n",
        "\n",
        "- Points are **spread out**, **meaningfully different**.\n",
        "\n",
        "We want to ignore directions where points are:\n",
        "\n",
        "- Almost identical,\n",
        "- Differences look like **noise** or **redundancy**.\n",
        "\n",
        "**High variance direction:**\n",
        "\n",
        "- Data points differ a lot ‚Üí **more information**.\n",
        "\n",
        "**Low variance direction:**\n",
        "\n",
        "- Points are almost the same ‚Üí likely **noise** or **redundant** information.\n",
        "\n",
        "So PCA searches for directions that:\n",
        "\n",
        "- **Maximize variance**  \n",
        "  ‚Üí preserve **structure, distances, separation** in the data.\n",
        "\n",
        "\n",
        "### 6.3 Example with Two Classes (Green vs Red Points)\n",
        "\n",
        "Imagine 2D data with two classes:\n",
        "\n",
        "- Green points (class A)\n",
        "- Red points (class B)\n",
        "\n",
        "They are well separated **diagonally**, but depending on the projection axis:\n",
        "\n",
        "- **Project onto a good direction**:\n",
        "  - Green and red points remain **separated**.\n",
        "  - Distances between classes are **preserved**.\n",
        "\n",
        "- **Project onto a bad direction**:\n",
        "  - Green and red points **collapse** close together.\n",
        "  - A model finds it **harder** to separate them.\n",
        "\n",
        "So, **maximizing variance** along the chosen direction helps:\n",
        "\n",
        "- Maintain **distances and class separations**,  \n",
        "- So that even after dimensionality reduction, the ML model still ‚Äúsees‚Äù the **structure**.\n",
        "\n",
        "That‚Äôs why PCA‚Äôs optimization goal is:\n",
        "\n",
        "- **Find directions (vectors) that maximize the variance of projected data.**\n",
        "\n",
        "Formally, for each direction \\( w \\):\n",
        "\n",
        "\\[\n",
        "\\max_{\\|w\\| = 1} \\operatorname{Var}(X w)\n",
        "\\]\n",
        "\n",
        "This leads to **eigenvectors of the covariance matrix**:\n",
        "\n",
        "- These eigenvectors are the **principal components**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Formal PCA Algorithm (Step-by-Step)\n",
        "\n",
        "Assume your data matrix \\( X \\) has shape \\((n_{\\text{samples}}, n_{\\text{features}})\\).\n",
        "\n",
        "### Step 1 ‚Äì Standardize the Features\n",
        "\n",
        "PCA is **scale-sensitive**, so:\n",
        "\n",
        "1. Subtract the **mean** from each feature.\n",
        "2. Optionally divide by the **standard deviation**.\n",
        "\n",
        "Result: each feature has **mean 0**, and comparable scale.\n",
        "\n",
        "### Step 2 ‚Äì Compute the Covariance Matrix\n",
        "\n",
        "For mean-centered data \\( X \\) of shape \\( n \\times d \\):\n",
        "\n",
        "\\[\n",
        "\\Sigma = \\frac{1}{n - 1} X^\\top X\n",
        "\\]\n",
        "\n",
        "- \\( \\Sigma \\) is a \\( d \\times d \\) **covariance matrix**.\n",
        "- Entry \\( \\Sigma_{ij} \\) is the covariance between feature \\( i \\) and \\( j \\).\n",
        "\n",
        "### Step 3 ‚Äì Eigen Decomposition\n",
        "\n",
        "Compute **eigenvalues** and **eigenvectors** of \\( \\Sigma \\):\n",
        "\n",
        "\\[\n",
        "\\Sigma v_k = \\lambda_k v_k\n",
        "\\]\n",
        "\n",
        "- \\( v_k \\): eigenvector (a **direction** in feature space).\n",
        "- \\( \\lambda_k \\): eigenvalue (variance **along that direction**).\n",
        "\n",
        "### Step 4 ‚Äì Sort by Eigenvalues\n",
        "\n",
        "- Sort eigenvectors in **decreasing** order of their eigenvalues.\n",
        "- Eigenvector with the **largest** eigenvalue ‚Üí **PC1**.\n",
        "- Next largest ‚Üí **PC2**, and so on.\n",
        "\n",
        "### Step 5 ‚Äì Choose Number of Components \\(K\\)\n",
        "\n",
        "Decide how many components to keep.\n",
        "\n",
        "Common strategies:\n",
        "\n",
        "- Keep enough components to explain, e.g., **95% of total variance**.\n",
        "- Or choose \\( K \\) manually (e.g., \\(K = 2\\) or \\(3\\) for **visualization**).\n",
        "\n",
        "### Step 6 ‚Äì Project Original Data\n",
        "\n",
        "Let \\( W \\) be the matrix of top \\(K\\) eigenvectors (shape \\( d \\times K \\)).\n",
        "\n",
        "Transform data:\n",
        "\n",
        "\\[\n",
        "Z = X W\n",
        "\\]\n",
        "\n",
        "- \\( Z \\) has shape \\( n \\times K \\): data in the new **K-dimensional PCA space**.\n",
        "- Each **column of \\(Z\\)** is one **principal component**.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Where PCA Fits in an ML Pipeline\n",
        "\n",
        "Typical **supervised ML pipeline**:\n",
        "\n",
        "1. **Train‚Äìtest split** your data.\n",
        "2. **Scale / standardize** your features.\n",
        "3. **Fit PCA only on training features** \\( X_{\\text{train}} \\):\n",
        "\n",
        "   ```python\n",
        "   pca.fit(X_train_scaled)\n",
        "\n",
        "4. Transform both train and test using the fitted PCA:\n",
        "    ```python\n",
        "    X_train_pca = pca.transform(X_train_scaled)\n",
        "    X_test_pca  = pca.transform(X_test_scaled)\n",
        "\n",
        "5. Train your model (e.g. logistic regression, SVM) on X_train_pca.\n",
        "\n",
        "6. Evaluate on X_test_pca.\n",
        "\n",
        "Important:\n",
        "\n",
        ". Never fit PCA on the full dataset before splitting.\n",
        "\n",
        ". That would leak information from test into train.\n",
        "\n",
        ". Treat PCA as a preprocessing step that is learned only from training data.\n",
        "\n",
        "---\n",
        "## 9. Practical Example (Conceptual) ‚Äì Handwritten Digits\n",
        "\n",
        "1. Assume you have a dataset of handwritten digits (MNIST-like):\n",
        "\n",
        "    Each image:\n",
        "    28\n",
        "    √ó\n",
        "    28\n",
        "    28√ó28 pixels.\n",
        "\n",
        "    . After flattening: 784 features per image.\n",
        "\n",
        "    . This is high-dimensional.\n",
        "\n",
        "2. Pipeline:\n",
        "\n",
        "    Flatten images ‚Üí vectors of length 784.\n",
        "\n",
        "    Standardize the features.\n",
        "\n",
        "3. Apply PCA:\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=50)  # or use explained variance to choose\n",
        "pca.fit(X_train_scaled)\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_pca  = pca.transform(X_test_scaled)\n",
        "```\n",
        "\n",
        "    . Now each image is represented by 50 numbers instead of 784.\n",
        "\n",
        "4. Train a classifier (logistic regression, SVM, etc.) on these 50D vectors.\n",
        "\n",
        "5. Benefits:\n",
        "\n",
        "    . Much faster training and prediction.\n",
        "\n",
        "    . Often similar or better generalization (less overfitting).\n",
        "\n",
        "    . You can visualize digits in 2D/3D by taking 2 or 3 principal components.\n",
        "\n",
        "## 10. Summary ‚Äì Key Takeaways\n",
        "\n",
        "### Problem\n",
        "- Too many features ‚Üí **curse of dimensionality**\n",
        "  - Worse model performance\n",
        "  - Higher computation cost\n",
        "\n",
        "---\n",
        "\n",
        "### Dimensionality Reduction Approaches\n",
        "1. **Feature Selection**\n",
        "   - Drop some original features\n",
        "   - No new features created\n",
        "\n",
        "2. **Feature Extraction**\n",
        "   - Create new features from existing ones\n",
        "   - Information is redistributed, not discarded directly\n",
        "\n",
        "---\n",
        "\n",
        "### PCA (Principal Component Analysis)\n",
        "- **Unsupervised** (does not use labels)\n",
        "- A **feature extraction** technique\n",
        "- **Linear** method\n",
        "- Old, stable, and widely used in practice\n",
        "\n",
        "---\n",
        "\n",
        "### What PCA Does\n",
        "- Finds new **orthogonal axes** called *principal components*\n",
        "- Each component is a **linear combination** of original features\n",
        "- Components are ordered by **variance**:\n",
        "  - **PC1** ‚Üí highest variance\n",
        "  - **PC2** ‚Üí second highest\n",
        "  - and so on\n",
        "- Keep only the first **K components**\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "- Like a photographer choosing the best angle to capture a **3D scene in a 2D photo**\n",
        "- Or combining *rooms* + *bathrooms* into a single **‚Äúsize‚Äù** feature\n",
        "\n",
        "---\n",
        "\n",
        "### Why Variance Matters\n",
        "- High-variance directions:\n",
        "  - Capture meaningful differences between data points\n",
        "  - Preserve structure and potential class separation\n",
        "- PCA explicitly selects directions that **maximize variance**\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation Notes\n",
        "- **Always standardize features** before applying PCA\n",
        "- **Fit PCA on training data only**\n",
        "- Use the fitted PCA model to:\n",
        "  - Transform training data\n",
        "  - Transform test data\n",
        "- Choose number of components using:\n",
        "  - Explained variance ratio\n",
        "  - Domain requirements\n"
      ],
      "metadata": {
        "id": "ULwWvCGpUOia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART - 2\n",
        "# Principal Component Analysis (PCA) ‚Äì Sections 5‚Äì10\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Geometric PCA: Rooms + Bathrooms ‚Üí One ‚ÄúSize‚Äù Feature\n",
        "\n",
        "Consider a housing dataset with two highly correlated features:\n",
        "\n",
        "- `rooms` = number of rooms  \n",
        "- `bathrooms` = number of bathrooms  \n",
        "\n",
        "Typically: more rooms ‚áí more bathrooms. So points in 2D (rooms vs bathrooms) lie roughly along a diagonal line.\n",
        "\n",
        "### 5.1 What PCA Does in This 2D Case\n",
        "\n",
        "If you plot:\n",
        "\n",
        "- x-axis: rooms  \n",
        "- y-axis: bathrooms  \n",
        "\n",
        "The point cloud will be an elongated diagonal cluster.\n",
        "\n",
        "PCA will:\n",
        "\n",
        "- **Rotate the axes** so that:\n",
        "  - **PC1** lies along the direction of **maximum spread** (roughly the diagonal).\n",
        "  - **PC2** is **perpendicular** to PC1.\n",
        "\n",
        "So new axes:\n",
        "\n",
        "- **PC1** ‚âà ‚Äúoverall **size / space**‚Äù of the flat  \n",
        "  (a linear combination of rooms + bathrooms)\n",
        "- **PC2** ‚âà small leftover variation  \n",
        "  (noise or minor deviations from perfect correlation)\n",
        "\n",
        "Now:\n",
        "\n",
        "- **Variance along PC1** is very large.  \n",
        "- **Variance along PC2** is small.\n",
        "\n",
        "So we can:\n",
        "\n",
        "- **Keep only PC1** (the important direction)  \n",
        "- **Drop PC2** (mostly noise / redundant info)\n",
        "\n",
        "We reduced:\n",
        "\n",
        "- From 2D ‚Üí 1D\n",
        "\n",
        "### 5.2 Interpretation\n",
        "\n",
        "- We converted **two correlated features** (rooms, bathrooms) into **one main feature**: a ‚Äúsize-like‚Äù direction.\n",
        "- That new feature is a **linear combination** of rooms and bathrooms.\n",
        "- This is **feature extraction via PCA** (not feature selection ‚Äî we created a new feature).\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Why PCA Cares About Variance\n",
        "\n",
        "You repeatedly see phrases like:\n",
        "\n",
        "- ‚ÄúPick axis with **maximum spread / variance**‚Äù\n",
        "- ‚Äú**Maximize variance** along principal components‚Äù\n",
        "\n",
        "Question: **Why is variance so important?**\n",
        "\n",
        "---\n",
        "\n",
        "### 6.1 Mean vs Variance ‚Äì Quick Recap\n",
        "\n",
        "For data points \\(x_1, x_2, \\dots, x_n\\):\n",
        "\n",
        "- **Mean**:\n",
        "\n",
        "\\[\n",
        "\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i\n",
        "\\]\n",
        "\n",
        "- **Variance**:\n",
        "\n",
        "\\[\n",
        "\\mathrm{Var}(X) = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2\n",
        "\\]\n",
        "\n",
        "- **Standard deviation**:\n",
        "\n",
        "\\[\n",
        "\\sigma = \\sqrt{\\mathrm{Var}(X)}\n",
        "\\]\n",
        "\n",
        "Example:\n",
        "\n",
        "- Dataset A: \\(-5, 0, +5\\)  \n",
        "- Dataset B: \\(-10, 0, +10\\)\n",
        "\n",
        "Both have:\n",
        "\n",
        "- Mean = 0  \n",
        "\n",
        "But:\n",
        "\n",
        "- B is more spread out ‚áí **larger variance**\n",
        "\n",
        "So:\n",
        "\n",
        "- **Mean** tells you the **center**.  \n",
        "- **Variance** tells you **how spread out** values are around the center.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2 Variance as ‚ÄúInformation‚Äù\n",
        "\n",
        "In PCA, we want to keep directions where points are:\n",
        "\n",
        "- **Spread out**,  \n",
        "- **Meaningfully different**,  \n",
        "\n",
        "and ignore directions where they are:\n",
        "\n",
        "- Almost identical,  \n",
        "- Differences are mostly **noise**.\n",
        "\n",
        "Heuristics:\n",
        "\n",
        "- **High variance direction**  \n",
        "  ‚áí Data points differ a lot ‚áí **more information**.\n",
        "- **Low variance direction**  \n",
        "  ‚áí Points almost the same ‚áí likely **noise or redundancy**.\n",
        "\n",
        "So PCA searches for directions that:\n",
        "\n",
        "- **Maximize variance**  \n",
        "  ‚áí preserve structure, distances, and separations as much as possible after projection.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3 Example With Two Classes (Green vs Red Points)\n",
        "\n",
        "Imagine 2D data with two classes:\n",
        "\n",
        "- Green points (class A)  \n",
        "- Red points (class B)\n",
        "\n",
        "They are well separated **diagonally**, but:\n",
        "\n",
        "- If you project onto a bad axis (e.g., horizontal), the two classes may **overlap** heavily.\n",
        "- If you project onto a good axis (near the diagonal), the two classes stay **well separated**.\n",
        "\n",
        "**Good projection direction:**\n",
        "\n",
        "- Green and red remain apart.  \n",
        "- Distances between classes are preserved.\n",
        "\n",
        "**Bad projection direction:**\n",
        "\n",
        "- Green and red collapse close together.  \n",
        "- Model finds it hard to separate them.\n",
        "\n",
        "So:\n",
        "\n",
        "- Maximizing **variance along the chosen direction** often:\n",
        "  - Maintains distances,  \n",
        "  - Maintains class separation,  \n",
        "  - Preserves the **structure** of the data.\n",
        "\n",
        "Therefore PCA‚Äôs optimization goal is:\n",
        "\n",
        "> Find directions (vectors) along which the **variance of projected data is maximal**.\n",
        "\n",
        "Mathematically, for each direction \\(w\\):\n",
        "\n",
        "- Constraint: \\(\\lVert w \\rVert = 1\\)\n",
        "- Objective: maximize \\(\\mathrm{Var}(Xw)\\)\n",
        "\n",
        "This leads to:\n",
        "\n",
        "- **Eigenvectors** of the **covariance matrix**  \n",
        "  = the **principal components**.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Formal PCA Algorithm (Step-by-Step)\n",
        "\n",
        "Assume your data matrix \\(X\\) has shape:\n",
        "\n",
        "- \\(X \\in \\mathbb{R}^{n_{\\text{samples}} \\times n_{\\text{features}}}\\)\n",
        "\n",
        "We‚Äôll denote:\n",
        "\n",
        "- \\(n = n_{\\text{samples}}\\)  \n",
        "- \\(d = n_{\\text{features}}\\)\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1 ‚Äì Standardize the Features\n",
        "\n",
        "PCA is **scale dependent**. If one feature has much larger scale, it can dominate.\n",
        "\n",
        "So:\n",
        "\n",
        "1. Subtract mean from each feature (mean centering).\n",
        "2. Optionally divide by standard deviation (standardization).\n",
        "\n",
        "Result:\n",
        "\n",
        "- Each feature has mean ‚âà 0 and comparable scale.\n",
        "\n",
        "You typically do this with `StandardScaler` in sklearn.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2 ‚Äì Compute the Covariance Matrix\n",
        "\n",
        "For mean-centered data \\(X \\in \\mathbb{R}^{n \\times d}\\) (each column is a feature):\n",
        "\n",
        "\\[\n",
        "\\Sigma = \\frac{1}{n - 1} X^\\top X\n",
        "\\]\n",
        "\n",
        "- \\(\\Sigma \\in \\mathbb{R}^{d \\times d}\\) is the **covariance matrix**.\n",
        "- Entry \\(\\Sigma_{ij}\\) = covariance between **feature i** and **feature j**.\n",
        "- Diagonal entries = **variances** of individual features.\n",
        "- Off-diagonal entries = **covariances** (how two features change together).\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3 ‚Äì Eigen Decomposition of Covariance Matrix\n",
        "\n",
        "Compute eigenvalues and eigenvectors of \\(\\Sigma\\):\n",
        "\n",
        "\\[\n",
        "\\Sigma v_k = \\lambda_k v_k\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(v_k\\) = eigenvector (a direction in feature space).  \n",
        "- \\(\\lambda_k\\) = eigenvalue (variance along that direction).\n",
        "\n",
        "In words:\n",
        "\n",
        "- Each eigenvector \\(v_k\\) is a **candidate axis**.  \n",
        "- Each eigenvalue \\(\\lambda_k\\) tells how much **variance** lies along that axis.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4 ‚Äì Sort by Eigenvalues\n",
        "\n",
        "Sort:\n",
        "\n",
        "- Eigenvectors by **decreasing eigenvalues**.\n",
        "\n",
        "Then:\n",
        "\n",
        "- Eigenvector with **largest eigenvalue** ‚áí **PC1** (principal component 1).\n",
        "- Next largest ‚áí **PC2**.\n",
        "- And so on.\n",
        "\n",
        "So components are **ordered** by how much variance they explain.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 5 ‚Äì Choose Number of Components \\(K\\)\n",
        "\n",
        "Decide how many principal components you want to keep.\n",
        "\n",
        "Common strategies:\n",
        "\n",
        "- Choose \\(K\\) so that cumulative explained variance ‚â• e.g. **95%**.  \n",
        "- Or choose \\(K\\) manually (e.g. 2 or 3 for visualization, 50 for MNIST etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### Step 6 ‚Äì Project Original Data\n",
        "\n",
        "Let \\(W\\) be the matrix of top \\(K\\) eigenvectors:\n",
        "\n",
        "- \\(W \\in \\mathbb{R}^{d \\times K}\\)\n",
        "- Columns of \\(W\\) are \\(v_1, v_2, \\dots, v_K\\).\n",
        "\n",
        "Transform data:\n",
        "\n",
        "\\[\n",
        "Z = X W\n",
        "\\]\n",
        "\n",
        "- \\(Z \\in \\mathbb{R}^{n \\times K}\\) is the data in the new **K-dimensional PCA space**.\n",
        "- Each **column** of \\(Z\\) = one principal component.\n",
        "- Each **row** of \\(Z\\) = the low-dimensional representation of a sample.\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Where PCA Fits in an ML Pipeline\n",
        "\n",
        "**Short answer:** PCA is a preprocessing step (unsupervised feature extraction) you apply *after* train/test split and *only* fit on training data. Use it when you need to reduce dimensionality for speed, storage, visualization, or to reduce overfitting.\n",
        "\n",
        "## Typical place in the pipeline (ordered)\n",
        "\n",
        "1. **Raw data collection**\n",
        "2. **Cleaning & basic imputations** (missing values, outliers handling)\n",
        "3. **Train / validation / test split** ‚Üê *critical* (no leaking)\n",
        "4. **Preprocessing on features** (scaling, encoding categorical variables, etc.)\n",
        "5. **PCA fit on X_train only** ‚Üí `pca.fit(X_train_scaled)`\n",
        "6. **Transform** both train & test using the fitted PCA: `X_train_pca = pca.transform(X_train_scaled)` and `X_test_pca = pca.transform(X_test_scaled)`\n",
        "7. **Train ML model** on `X_train_pca` and evaluate on `X_test_pca`\n",
        "8. **(Optional) Tune number of components (K) using cross-validation inside a pipeline**\n",
        "\n",
        "## Why *fit PCA only on training data*?\n",
        "\n",
        "* Fitting PCA on the whole dataset leaks information from the test set into the training process (data leakage). That biases evaluation (optimistic performance).\n",
        "* PCA learns components that depend on the distribution; the test set must remain unseen during that learning.\n",
        "\n",
        "## Practical tips & gotchas\n",
        "\n",
        "* **Always scale** features when they have different units. PCA uses variance ‚Äî features with large scale dominate otherwise. Use `StandardScaler` or similar.\n",
        "* **Categorical features**: convert to numeric before PCA (one‚Äëhot / embedding). One-hot blows up dimensionality and often should be handled differently (feature selection, embeddings, target encoding) before PCA.\n",
        "* **Supervised vs unsupervised**: PCA is unsupervised ‚Äî it ignores labels. If you need label-aware reduction use supervised alternatives (LDA, supervised PCA, feature selection with model importance).\n",
        "* **Non-linear structure**: if data lies on non-linear manifolds, linear PCA can fail; consider Kernel PCA, t-SNE (visualization only), UMAP, or autoencoders.\n",
        "* **Interpretability**: PCA components are linear combinations of original features; loadings tell you which features contribute most. But the components themselves may be harder to explain.\n",
        "* **Model training speed**: reducing dimensions often speeds up training and inference, and reduces memory.\n",
        "* **Cross-validation**: include PCA inside a `Pipeline` so `fit/transform` happen correctly inside each CV fold. Example in scikit-learn: `Pipeline([('scaler',StandardScaler()),('pca',PCA(n_components=K)),('clf',LogisticRegression())])`.\n",
        "\n",
        "## Example scikit-learn snippets\n",
        "\n",
        "**Fit PCA on training data and transform**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=0.95)  # keep 95% variance (or n_components=int)\n",
        "pca.fit(X_train_scaled)\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_pca  = pca.transform(X_test_scaled)\n",
        "```\n",
        "\n",
        "**Use PCA inside cross‚Äëvalidated pipeline (no leakage)**\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=50)),\n",
        "    ('clf', LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy')\n",
        "```\n",
        "\n",
        "**Choose K by explained variance**\n",
        "\n",
        "```python\n",
        "pca = PCA().fit(X_train_scaled)\n",
        "cum_var = pca.explained_variance_ratio_.cumsum()\n",
        "# Pick smallest K where cum_var[K-1] >= 0.95\n",
        "```\n",
        "\n",
        "# 9. Practical Example (Conceptual + code sketch)\n",
        "\n",
        "### Problem: MNIST-ish handwritten digits (conceptual)\n",
        "\n",
        "* Each image is 28√ó28 ‚Üí 784 features (pixels). That‚Äôs high‚Äëdimensional and slow to train on.\n",
        "\n",
        "**Goal:** compress each image to 50 numbers using PCA and train a classifier on the compressed representation.\n",
        "\n",
        "### Why this helps\n",
        "\n",
        "* **Less compute:** 50 features vs 784 ‚Üí faster training, smaller models.\n",
        "* **Less overfitting:** noise / irrelevant pixel variation often captured in low-variance directions ‚Äî dropping them reduces overfitting.\n",
        "* **Visualization:** you can visualize the dataset in 2‚Äì3 PCs to inspect class overlap.\n",
        "\n",
        "### Step-by-step (conceptual)\n",
        "\n",
        "1. Flatten each image into a 784‚Äëlength vector.\n",
        "2. Split data into train / test.\n",
        "3. Standardize pixels (mean center; optionally divide by std).\n",
        "4. Fit PCA on X_train and choose n_components (e.g., 50 or `n_components=0.95`).\n",
        "5. Transform train & test with the fitted PCA.\n",
        "6. Fit a classifier (e.g., logistic regression, SVM, or small neural net) on the PCA features.\n",
        "7. Evaluate on the PCA-transformed test set.\n",
        "8. (Optional) Reconstruct images using `pca.inverse_transform` to inspect information loss.\n",
        "\n",
        "### Code sketch (scikit-learn style)\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=50)),\n",
        "    ('clf', LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "print('Train acc:', pipe.score(X_train, y_train))\n",
        "print('Test acc: ', pipe.score(X_test, y_test))\n",
        "\n",
        "# Inspect explained variance\n",
        "pca = pipe.named_steps['pca']\n",
        "print('Explained variance ratio (first 10):', pca.explained_variance_ratio_[:10])\n",
        "print('Cumulative var (50 comps):', pca.explained_variance_ratio_.cumsum()[-1])\n",
        "\n",
        "# Reconstruct a sample\n",
        "x_sample = X_test[0:1]\n",
        "x_sample_pca = pipe.named_steps['pca'].transform(pipe.named_steps['scaler'].transform(x_sample))\n",
        "x_recon = pipe.named_steps['pca'].inverse_transform(x_sample_pca)\n",
        "# reshape and plot x_sample and x_recon to see loss\n",
        "```\n",
        "\n",
        "### Toy numeric intuition (rooms + bathrooms ‚Üí size)\n",
        "\n",
        "* Suppose features: `rooms` and `bathrooms` are correlated. PCA finds a unit direction (PC1) roughly along the `size` axis.\n",
        "* Projecting `(rooms, bathrooms)` onto PC1 gives a single number that explains most variance (size-like). PC2 is orthogonal and small (leftover noise).\n",
        "* Result: 2D ‚Üí 1D with minimal loss of structure.\n",
        "\n",
        "### When PCA might **hurt**\n",
        "\n",
        "* If labels depend on *low-variance* directions (rare but possible), PCA can remove the discriminative signal. Example: two classes separated only on a subtle low-variance feature.\n",
        "* If features are categorical or sparse (e.g., one-hot text features), PCA on raw one-hot vectors often produces uninterpretable dense features. Consider alternatives.\n",
        "\n",
        "# 10. Summary ‚Äî Key Takeaways\n",
        "\n",
        "* **What PCA is:** an *unsupervised* linear feature‚Äëextraction method that finds orthogonal axes (principal components) ordered by variance.\n",
        "* **What it does:** rotates the coordinate axes to new directions that capture the largest variance; you can keep the top K components to reduce dimensionality.\n",
        "* **Why variance matters:** directions with high variance contain more information (differences between data points). PCA keeps those and discards low‚Äëvariance directions likely to be noise or redundancy.\n",
        "* **Where to use it:** when you need faster models, lower memory, visualization, or to reduce overfitting from very high-dimensional numeric data.\n",
        "* **How to use it correctly:**\n",
        "\n",
        "  * Always split data first (train/test).\n",
        "  * Fit PCA only on training data.\n",
        "  * Scale features before PCA.\n",
        "  * Put PCA inside the training `Pipeline` used for cross‚Äëvalidation to avoid leakage.\n",
        "* **Limitations:** linear method (won't capture nonlinear manifolds), unsupervised (ignores labels), may harm classification if discriminative signal lies in low-variance directions.\n",
        "\n",
        "**Quick checklist before applying PCA**\n",
        "\n",
        "* Are my features numeric and reasonably scaled? If not, fix them.\n",
        "* Is linear reduction acceptable? If not, try kernel PCA / autoencoders / UMAP.\n",
        "* Do I have enough data to estimate covariance reliably? Very small n with huge d is tricky.\n",
        "* Will dropping components remove label-relevant information? Validate with a pipeline and CV.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can: produce the same content but with ready‚Äëto‚Äërun Jupyter cells (copy‚Äëpaste), or add a short example notebook that reconstructs MNIST digits from principal components.\n"
      ],
      "metadata": {
        "id": "vloB0H39VzBr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbBLumFWQCR8"
      },
      "outputs": [],
      "source": []
    }
  ]
}